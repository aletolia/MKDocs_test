### 摘要

国际基准测试竞赛已成为图像分析方法比较性能评估的基础。然而，关于从这些竞赛中能学到什么的研究却很少。它们真的能推动科学进步吗？常见和成功的参与策略是什么？是什么使得一个解决方案优于竞争方法？为了解决文献中的这一空白，我们对 IEEE ISBI 2021 和 MICCAI 2021 范围内进行的所有 80 场竞赛进行了多中心研究。基于提交算法的综合描述及其排名的统计分析，以及参与策略的统计分析，揭示了获胜解决方案的共同特征。通常包括使用多任务学习（63%）和/或多阶段管道（61%），以及注重增强（100%）、图像预处理（97%）、数据管理（79%）和后处理（66%）。获胜团队的“典型”负责人通常是一名计算机科学家，拥有博士学位，在生物医学图像分析领域有五年的经验，并在深度学习方面有四年的经验。高度排名团队的两大核心开发策略脱颖而出：在方法设计中反映评估指标，以及专注于分析和处理失败案例。据组织者称，43% 的获胜算法超越了现有技术，但只有 11% 完全解决了各自领域的问题。我们的研究结果可以帮助研究人员（1）在解决新问题时改进算法开发策略，以及（2）关注本研究揭示的开放研究问题。

### 1. 引言

生物医学图像分析算法的验证通常通过所谓的挑战赛进行——这些是比较算法在解决特定问题的数据集上的性能的大型国际基准测试竞赛。近年来，不仅用于解决任务的机器学习（ML）模型的复杂性有所增加，挑战赛的科学影响力也显著提高，结果常常在著名期刊上发表（例如，[9, 28, 34, 41, 46]），获胜者在引用次数和（有时）高额奖金方面获得了巨大关注 [23]。然而，尽管有这些影响，但迄今为止，关于从挑战赛中能学到什么的研究却投入甚少。首先，我们发现文献中关于挑战赛中的当前常见实践以及批判性分析挑战赛是否真正产生科学进步的研究存在明显的空白。其次，尽管最近的工作解决了从挑战赛中得出有意义结论的问题 [29, 49]，但仍然在很大程度上不清楚什么使获胜者成为最佳，以及因此在应对新的挑战或问题时，什么是良好的策略。具体问题是多方面的，例如，目前获胜解决方案中使用了哪些特定的训练范式？实现泛化的最成功策略是什么？是否有必要邀请领域专家或在大团队中工作？尽管可以通过消融研究来解决某些问题，即研究移除 ML 模型组件的效果，但它们主要缺点是只能提供对提交的解决方案的洞察，而不能提供对潜在策略的洞察。此外，它们通常只允许研究解决方案的少数方面，并且会产生巨大的碳足迹。

为克服这些问题，我们选择了一种方法，允许我们系统地评估与生物医学图像分析竞赛相关的所有上述问题。在此背景下，Helmholtz 成像孵化器（HI）和医学图像计算与计算机辅助干预（MICCAI）生物医学图像分析挑战赛特别兴趣小组的成员设计了一系列综合性的国际调查，这些调查发给了参加 IEEE 国际生物医学成像研讨会（ISBI）2021 和国际 MICCAI 2021 会议竞赛的参与者、组织者和获奖者。通过与所有 80 场竞赛的组织者（100%，见附录 A 中的概述）合作，我们能够将算法设计决策和挑战参与策略与排名结果联系起来。基于研究数据，我们明确解决了三个研究问题：（RQ1）挑战赛参与的常见实践是什么？（RQ2）当前的竞赛是否产生科学进步？（RQ3）哪些策略是挑战赛获胜者的特点？

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202406101317392.png)

> 图 1. IEEE ISBI 2021 和 MICCAI 2021 挑战赛概述。在 35 个挑战赛（每个由一个预告图像和缩写表示）的框架下，共组织了 80 场专门设有排行榜的竞赛，详见附录 A。我们使用来自参与者、组织者和获胜者的数据，回答了本文的关键研究问题：（RQ1）挑战赛参与的常见实践是什么？（RQ2）当前的竞赛是否产生科学进步？（RQ3）哪些策略是挑战赛获胜者的特点？

### 2. 方法

根据生物医学图像分析挑战赛（BIAS）关于提高健康研究质量和透明度（EQUATOR）指南的定义，生物医学图像分析挑战赛被定义为“[...] 在生物医学图像分析领域的特定科学问题上的公开竞赛。一个挑战赛可能包括多个与多个任务相关的竞赛，参与的团队可能不同，并且为其生成单独的排名/排行榜/结果。”由于在机器学习（ML）社区中，挑战任务这个术语并不常见，我们将使用竞赛这个术语。挑战这个术语将保留用于在一个专门组织的框架下进行的一系列任务集合，以一个缩写表示（图 1）。在我们的分析中，我们针对挑战赛相关的三大主要群体，即（1）挑战赛参与者，（2）挑战赛组织者，以及（3）挑战赛获胜者。以下部分介绍了为解决相应的研究问题 RQ1-RQ3 而制定的方法。

#### 2.1. RQ1: 挑战赛参与的常见实践是什么？

为了调查当前生物医学图像分析挑战赛参与的常见实践，我们设计了一份调查问卷，针对挑战赛参与者，并分为五个部分，包括：（1）团队和所解决任务的一般信息，（2）专业知识和环境信息，（3）挑战赛策略，（4）算法特征，以及（5）其他信息（详细信息见第 3 节）。 

我们邀请了所有 IEEE ISBI 2021 挑战赛（6 个挑战赛中的 30 场竞赛 [1,2,12,35,40,42]）和所有 MICCAI 2021 挑战赛（29 个挑战赛中的 50 场竞赛 [3-6, 8, 10, 13, 14, 16, 18, 19, 21, 22, 26, 27, 32, 33, 36, 37, 43, 44, 48, 50, 51]）的组织者参与这项计划，并将我们与参与者联系起来（如果挑战赛隐私政策允许的话）或将调查链接分发给他们。我们为每个挑战赛创建了一个单独的调查网站，以便适应各个挑战赛的提交截止日期。为了避免调查响应中的偏差，参与者被要求在知道最终排名之前完成调查。在最多 168 个问题中，调查仅显示与具体情况相关的问题。IEEE ISBI 2021 的受访者的回答和反馈被用于改进 MICCAI 2021 的调查，因此不包括在第 3.1 节中呈现的结果中。对于允许组织者分享参与者联系信息的 20 个挑战赛，调查以封闭访问模式进行，这意味着参与者收到个人调查链接，并在必要时收到提醒。十五项调查以开放访问模式进行，意味着组织者负责分享相应调查的链接并发送提醒。在这些情况下，我们未被告知挑战赛参与者的数量，无法将响应数量与总数量联系起来。

#### 2.2. RQ2: 当前的竞赛是否产生科学进步？

组织者调查的重点是各自竞赛的发现，特别是是否取得了科学进步，如果有，在哪些领域取得了进展以及哪些开放问题仍然存在。为了更好地将各自竞赛置于背景中，我们还收集了与相关竞赛的一般信息。

#### 2.3. RQ3: 挑战赛获胜者的特点策略是什么？

基于最新神经网络方法的复杂性，包括众多相互依赖的设计参数，存在将竞赛成功归因于系统错误组件的风险。为了解为什么获胜者是最优秀的，我们将第 2.1 节的调查结果与竞赛的最终结果联系起来，并随后应用混合模型分析。考虑到相对于竞赛数量的大量参数，我们意识到参数差异可能无法达到统计显著性。因此，在第二步中，我们通过额外的调查明确询问挑战赛获胜者关于成功算法设计选择和策略。

**混合模型分析**  

为补偿由于对应特定竞赛的集群而导致的层次数据结构，使用了逻辑混合模型。第一步进行单变量分析，即分别调查每个变量对排名的影响。为了进一步考虑变量之间的潜在相互依赖性，添加了两个多变量分析。第一个分析的目的是调查影响获胜概率的策略，而第二个分析则侧重于评估影响排名前 30% 概率的策略。对于这两项分析，实施了逻辑混合模型。获胜策略被包括为固定效应，而挑战标识符被包括为随机效应。此外，允许某些策略在挑战赛中变化，特别是计算小时的总训练时间、用于分析数据和注释的时间以及用于分析失败案例的时间。在拟合模型之前，对具有高度变化幅度的变量进行了缩放。统计分析在 R 统计软件 [38]（v4.0.3，包：lme4 [7]）中完成。

**关于获胜策略的调查**  

对竞赛获胜者的调查分为三个主要部分，分别针对与获胜提交相关的设计决策、获胜竞赛的一般推荐策略以及获胜者的个人资料。

在第一部分中，我们询问获胜者提交方法的各种设计决策的重要性。这些设计决策包括：（1）训练范式的设计决策，如多任务学习或半监督学习的使用，（2）网络细节，如损失函数的选择，（3）模型初始化，特别是预训练，（4）数据使用，涵盖数据管理、增强、数据划分和采样等方面，（5）超参数，（6）集成，（7）后处理，以及（8）评估指标（见图 3）。对于每个设计决策，获胜者指定其方法（例如，他们是否进行了预训练，如果是，则基于哪些数据）并评估该设计选择对获胜挑战赛的重要性。我们还明确询问了获胜解决方案与竞争解决方案的区别以及成功的关键因素。

调查的第二部分调查了一般成功策略（独立于特定挑战赛）。为此，本文的几位已经赢得多次挑战赛的作者编制了策略列表（图 4）。获胜者被要求评估每个策略的重要性，并进一步补充列表。

最后，调查的第三部分涵盖了关于挑战赛获胜者个人资料的问题（图 2）。这对于那些未参加第 2.1 节原始调查的获胜者尤为重要。

### 3. 结果

根据所有 IEEE ISBI 2021（n = 30）和 MICCAI 2021（n = 50）竞赛的组织者的积极回应，共包括了 35 个挑战赛中的 80 场竞赛（图 1）。这些竞赛涵盖了语义分割、实例分割、图像级分类、跟踪、目标检测、配准和流水线评估等广泛的问题。

#### 3.1. 挑战赛参与的常见实践

根据封闭访问调查，挑战赛参与者的中位数（最小/最大）为 72%（11%/100%）参与了调查。总体而言，我们收到了 292 份完整的调查表，其中 249 份符合我们的纳入标准（即，针对 MICCAI 2021 改进的第二版调查，由主要开发者完成调查，且没有来自同一团队的重复响应）。所有调查方面的详细回复（包括所有参数的四分位距（IQR）和最小/最大值）都在白皮书 [15] 中提供。本节总结了部分答案。获胜者的个人资料如图 B.1 所示。

**基础设施和策略**  
知识交流是参与的最重要动机（70% 提到；受访者可以选择多个答案），其次是将自己的方法与他人进行比较的可能性（65%）、获得数据的机会（52%）、成为即将发表的挑战赛文章的一部分（50%）以及赢得挑战赛（42%）。只有 16% 的受访者认为奖项/奖金很重要。关于计算基础设施，只有 25% 的受访者认为他们的基础设施是瓶颈。绝大多数受访者使用图形处理单元（GPU）集群。所有模型在方法开发期间的总训练时间，包括失败模型，估计为中位数 267 GPU 小时，而最终提交的训练时间估计为中位数 24 GPU 小时。最流行的框架是用于方法实现的 PyTorch（76%）、用于数据分析的 NumPy（37%）和用于注释/参考数据分析的 NumPy（27%）。

最常见的发展方法（42%）是通过阅读相关文献并基于现有工作进行构建或修改。大多数（51%）估计最终解决方案的代码编辑行数在 10^3 量级。方法开发总共花费了中位数 80 个工作小时。受访者报告更多的是人为决策（中位数 60%），例如基于专业知识的参数设置，而不是经验决策（中位数 40%），例如通过网格搜索进行自动超参数调整。94% 的受访者使用了基于深度学习的方法。对于这些方法，大部分时间（最多允许三项选择）花费在选择与任务最匹配的一个或多个现有架构（45%）、配置数据增强（33%）、配置模板架构（例如，深度？有多少阶段/池化层？）（28%）、探索现有损失函数（25%）和集成（22%）。

调查显示，几乎三分之一的受访者没有足够的时间进行开发。其中大多数（65%）认为多几周的时间会有所帮助（几个月：18%，几天：14%）。

**算法特征**  
在基于深度学习的方法中，只有 9% 在最终解决方案中积极使用了额外的数据，即未提供给相应挑战赛的数据（注意，这不包括使用已经预训练的模型）。一个原因可能是一些挑战赛（24%）明确不允许使用外部数据。在那些利用外部数据的情况下，大多数使用了公共生物医学数据用于相同类型的任务（40%）、私人生物医学数据用于相同类型的任务（25%）或公共生物医学数据用于不同类型的任务（15%）。只有 5% 的情况下使用了非生物医学数据。如果使用了额外的数据，通常用于预训练（55%）和/或共同训练（50%）。

数据增强由 85% 的受访者应用。最常见的增强方式包括随机水平翻转（77%）、旋转（74%）、随机垂直翻转（62%）、对比度调整（49%）、缩放（48%）、裁剪（44%）、调整大小裁剪（35%）、噪声（34%）、弹性变形（26%）、颜色抖动（19%）和剪切（15%）。43% 的受访者报告数据样本过大，无法一次处理（例如，由于 GPU 内存限制）。这个问题主要通过基于补丁的训练（裁剪）（69%）、降采样到较低分辨率（37%）和/或通过后处理将 3D 分析任务解决为一系列 2D 分析任务（每 z 切片方法）（18%）来解决。最常见的损失函数是交叉熵（CE）损失（39%）、组合 CE 和 Dice 损失（32%）以及 Dice 损失（26%）。29% 的受访者使用了提前停止，12% 使用了预热。超过一半的受访者（52%）通过单一的训练：验证（：测试）分割进行内部评估。37% 的受访者在训练集上进行了 K 折交叉验证。6% 的受访者未进行任何内部评估。48% 的受访者应用了后处理步骤。50% 的受访者的最终解决方案是一个在所有可用数据上训练的单一模型。6% 提出了一个由多个相同模型组成的集成，每个模型在完整训练集上进行训练，但使用不同的初始化（随机种子）。21% 提出了由多个相同模型组成的集成，每个模型在随机抽取的训练集子集上进行训练（无论是否使用相同的种子）。9% 报告集成了多个不同模型，并在整个训练集上训练每个模型（不同的种子）。8% 集成了多个不同的模型，每个模型在随机抽取的训练集子集上进行训练（无论是否使用相同的种子）。如果使用多个模型，最终解决方案由中位数为 5 个模型组成。

#### 3.2. 挑战赛产生的科学进步的关键见解

根据挑战赛组织者的回复（n = 54），43% 的获胜算法超过了现有技术水平（图 2）。尽管在大多数竞赛中取得了显著（47%）或轻微（32%）进展，但只有 11% 的竞赛认为基础问题得到了完全解决。最大的进展见于新架构/架构组合（32%）、优化问题的表述（例如，新损失函数）（17%）和新增强策略（14%）。失败案例主要归因于特定成像条件（例如，图像模糊）（27%）、泛化问题（23%）和特定类别表现特别差（19%）。

根据多位组织者的回复，简单算法（例如，U-Net [17] / nnU-Net [24]）优于复杂算法的趋势继续存在。2021 年的一个显著特点是，许多竞赛提供了通常不可用的额外信息，如域泛化的医院标识符、表示标签不确定性的多个专家分割或重建问题中的 k 空间数据。然而，参与者未能利用这些额外数据来提升性能。视频分析中的时间数据也是如此，尽管组织者推测基于帧的分析不足以应对这一问题。

多位组织者还报告了方法的异质性不足。提交的方法往往表现相似（例如，在归一化分数中仅在小数点后第四位有所不同）。值得注意的是，一些运行多年的竞赛观察到相较于前几年有显著改进，有时甚至超越了人类表现。关于计算方面，一个获胜方法超越了现有最先进的方法，实现了 19 倍的推理速度提升，并减少了 60% 的 GPU 内存消耗，同时保持了可比的准确性。

根据我们的研究，泛化仍然是一个主要问题。一项模拟“真实环境”部署的挑战发现，模型在 21 个测试机构中的 3 个未能泛化。类似地，稀有类别的表现被报告为多个竞赛的核心问题。这在临床上具有重要意义，因为疾病通常对应于稀有类别。相关问题是多标签设置中检测多种情况的挑战性。最后，一些组织者报告了评估指标未能反映生物医学领域的兴趣。例如，尽管像素级表现有时被报告为显著，但实例/病例级表现（通常在生物医学上更相关）并未有显著改进。

在 4% 的情况下观察到了作弊现象。这与用不同用户账户提交相似方法的过多数量或试图从提交平台检索测试集有关。在这些情况下，参与者被排除在竞赛、排名和/或发表之外。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202406101322874.png)

> Figure 2. Key insights provided by the organizers of IEEE ISBI 2021 and MICCAI 2021 challenges.

#### 3.3. 获胜策略的关键见解

在比较获胜者与其他参与者时，有几个显著差异。首先，获胜者更有决心赢得挑战赛（64% vs. 40%）。大多数获胜的主要开发者拥有博士学位（41%），而大多数未获胜的主要开发者的最高学位是硕士学位（47%）。此外，虽然只有 66% 的其他参与者认为有足够的开发时间，但 86% 的获胜者同意这一说法。获胜者在决定提交之前花费了 120 小时（例如，在方法开发、数据和注释分析上），而其他参与者为 56 小时，并且决定提交的时间比其他参与者早一周（提交前 3 周 vs. 2 周）。值得注意的是，获胜者花费在失败分析上的时间是其他参与者的两倍（方法开发总工作时间的 10% vs. 5%）。与非获胜者相比，获胜者在基于随机种子、数据分割和异质模型上的集成使用分别为 5.6 倍、1.7 倍和 2.5 倍。

根据单变量混合模型分析，八个参数在获胜者和非获胜者之间提供了显著的统计差异（p < 0.05）：（1）团队成员中开发人员/工程师的数量，（2）计划提交结果前的投资时间，（3）数据预处理/增强花费的时间，（4）使用专业管理的 GPU 集群，（5）用于方法开发的方法，（6）架构类型，（7）在寻找超参数时考虑评估挑战的指标，以及（8）使用的增强方式。然而，当进行多个独立测试时，预计有 5% 的结果纯粹是偶然显著的（在 5% 的显著性水平下）。在纠正了这种所谓的测试多重性后，我们没有获得统计显著性差异。基于图像分析专家识别的变量选择的多变量模型分析表明，获胜者与非获胜者相比，获胜挑战的意愿是唯一具有 p < 0.05 的参数（64% vs. 40%）。类似地，在最佳 30% 与其余分析中，寻找超参数时考虑评估挑战的指标参数被识别出来。值得注意的是，尽管 72% 的高响应率，第 2.1 节调查覆盖的获胜者仅为 22 人。识别赢得挑战赛的重要贡献者的低统计能力可能是未能获得统计显著性的原因。因此，我们在结果公布后额外询问了竞赛获胜者关于关键设计决策和策略的情况。回复（n = 38）分别涵盖了 IEEE ISBI 2021 和 MICCAI 2021 挑战赛的 67% 和 62%，并总结在图 3 和图 4 中。

![](https://raw.githubusercontent.com/aletolia/Pictures/main/202406101324801.png)

> 图 3. 设计决策的重要性
> 
> 图 3 展示了 IEEE ISBI 2021 和 MICCAI 2021 竞赛中基于神经网络的获胜提交的设计决策重要性，由（团队）负责人进行评级，并按最高投票百分比（至关重要：深蓝色）排序。投票仅在使用相应设计的受访者中进行。“应用者”表示使用相应设计的受访者百分比。

如图 3 所述，最常用的训练流水线是多任务设计（63%）和多阶段流水线（61%）。如果应用了多阶段流水线，获胜者认为这种策略对赢得挑战赛至关重要。预训练主要在监督方式下进行，使用领域内数据（55%）或通用数据（例如，ImageNet）（61%）。然而，领域内数据的使用被认为更加重要。如前所述，许多竞赛不允许使用外部数据（根据第 2.2 节的调查，24%）。与数据使用相关的最常见设计决策是预处理（97%）、增强（100%）、数据分割（超出竞赛提供的分割，例如，用于交叉验证）（89%）、数据管理（例如，注释清理）（79%）和数据采样（58%）。在询问获胜者成功的关键因素时（自由文本），突出的一个方面是建立良好的内部验证策略，包括基准模型的仔细选择和适当的验证测试。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202406101325611.png)

> 图 4. IEEE ISBI 2021 和 MICCAI 2021 竞赛获胜者认为的获胜策略，按“至关重要”（深蓝色）和“非常重要”（浅蓝色）类别的总和排序。每个策略的重要性分布（从左到右：完全不重要、几乎不重要、中等重要、非常重要、至关重要）被描绘出来。

关于一般策略（图 4），分析和处理失败案例、了解最新技术和在方法设计中反映评估指标的策略被评为最高。自由文本回答中进一步推荐的策略是多样化的，包括（1）在模型集成中包括非深度学习方法，（2）明确确定时间管理策略，（3）测试时增强，以及（4）优先选择成熟的架构而不是全新的热门机器学习方法。

### 4. 讨论

据我们所知，这项研究是首次系统且大规模地考察生物医学图像分析竞赛，重点是科学界可以从中学到什么。基于对两个主要会议范围内的 80 场竞赛的全面调查和统计分析，它提供了对挑战赛参与者的常见实践、竞赛带来的科学进步、未解决的问题以及关键获胜策略的前所未有的见解。

**关于常见参与实践（RQ1）的新见解**  
知识交流是主要的参与动机。这可能与平台如 Kaggle 上存在差异，在 Kaggle 上，奖金和获得高排名预计更为重要 [45]。令我们惊讶的是，只有少部分参与者认为计算能力限制是瓶颈。同样令人惊讶的是，只有少数参与者在训练集上进行了 K 折交叉验证和集成。

**竞赛带来了显著的科学进步（RQ2）**  
然而，只有一小部分当前竞赛所解决的图像分析问题被认为是完全解决的（附录 C）。本研究确定的未解决研究问题包括：（1）如何更好地将元信息整合到神经网络解决方案中？（2）如何有效利用生物医学视频分析中的时间信息？（3）如何实现设备、协议和站点间的泛化？（4）如何制定更能反映生物医学领域需求的性能指标？特别值得注意的是，反映评估指标在挑战赛设计中的重要性被认为是获胜的关键策略之一。与最近的文献一致 [20, 25, 39, 47]，这表明普遍努力主要集中在过度拟合当前指标上，而不是解决底层领域问题。尽管当前已有针对这一问题的举措 [30]，但我们的结果表明挑战赛组织者应更关注确保实际生物医学需求在竞赛设计中得到反映。

**我们的研究揭示了特别成功的算法设计选择（图 3）和获胜的一般策略（图 4）（RQ3）**  
为了报告负面结果，我们也包括了混合模型分析的结果，尽管在校正测试多重性后缺乏统计显著性。鉴于相对于从算法设计和策略中提取的参数数量（> 100），数据集（80 场竞赛）的相对较小，我们假设统计显著性的缺乏主要归因于样本量小。

**我们研究的局限性**  
可以认为我们仅覆盖了某一年份的 IEEE ISBI 和 MICCAI 挑战赛。然而，先前的研究显示，这些会议范围内进行的竞赛涵盖了大多数生物医学图像分析竞赛 [29]。进一步的局限性可以被认为是与调查研究的一般局限性有关 [11]，包括自报告数据的不确定性和由分类变量预选产生的潜在偏见。最后，用单一问卷解决挑战赛的异质性并非易事。例如，由于公共生物医学数据集的稀缺性，使用域内相似数据集可能并不总是可行。同样，研究人员可能认为集成是一项关键策略，但由于计算能力限制，无法训练和优化多个处理视频、3D 或 4D 数据的模型。为了补偿在第 2.1 节和第 2.2 节中提出的调查设计中的这种效果，我们额外询问了获胜者的一般推荐策略（图 4）。答案反映了普遍推荐与可行性之间的差异。例如，大多数获胜者建议在团队中融入生物学家/临床医生，但他们自己并未这样做。

尽管存在上述局限性，我们的发现有潜力影响挑战赛中的众多利益相关者。首先，生物医学图像分析研究人员和开发人员可以“站在巨人的肩膀上”（竞赛获胜者）来改进解决新问题时的算法开发策略。其次，未来的挑战赛组织者可以仔细调整他们的设计，以解决本研究揭示的开放问题。这将包括关注案例/实例级别而不是像素/体素级别以反映生物医学需求，制定反映生物医学需求的指标（见下文），以及允许改进算法在稀有类别上表现和跨领域泛化能力的数据集设计。鉴于绝大多数参与者认为时间限制而非计算能力是瓶颈，挑战赛时间表应受到严格审视。最后，更广泛的社区可以从我们确定的开放研究问题中受益（表 D.1）。

我们进行了首次系统分析生物医学图像分析竞赛，揭示了关于参与、组织和获胜的众多新见解。我们的工作可以为以下两方面铺平道路：（1）开发者在解决新问题时改进算法开发策略，以及（2）科学界将其活动集中于本研究揭示的开放问题。