{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4cc5a61",
      "metadata": {
        "id": "f4cc5a61"
      },
      "source": [
        "# 在计算病理学中用于癌症诊断的弱监督深度学习\n",
        "\n",
        "- 主讲人：\n",
        "   - Guillaume Jaume (gjaume@bwh.harvard.edu)\n",
        "   - 哈佛医学院和布里格姆妇女医院博士后研究员\n",
        "- 最初由Richard J. Chen (richardchen@g.harvard.edu)提出和撰写\n",
        "\n",
        "![](https://user-images.githubusercontent.com/10300839/232984533-c22822b8-df80-4b95-80e2-93dde2409bbf.png)\n",
        "\n",
        "**定义:**\n",
        "\n",
        "- *计算病理学（CPath）:* 基于细胞和组织的显微分析的计算方法，用于研究疾病。\n",
        "\n",
        "- *数字病理学:* 用于在数字环境中获取、管理和诊断病理玻片的一组工具和系统。\n",
        "\n",
        "- *全玻片图像（WSI）:* 使用扫描仪对玻片进行高分辨率数字化得到的图像。\n",
        "\n",
        "- *溴甘醇-威廉士（H&E）染色:* 组织学分析的参考染色方法，用于可视化细胞核（紫色）和细胞外信息以及细胞质（粉红色）。\n",
        "\n",
        "**背景:**\n",
        "\n",
        "计算病理学旨在利用基于人工智能的计算工具自动化、辅助和增强病理学临床实践。\n",
        "\n",
        "组织表型分型是计算病理学（CPATH）中的一个基本问题，用于表征癌症诊断、预后和治疗反应的组织病理学特征。与自然图像不同，全玻片成像是一个具有挑战性的计算机视觉领域，其中图像分辨率可以高达$150,000 \\times 150,000$像素（加载整个图像需要超过50 GB的内存）。\n",
        "\n",
        "为了解决这个计算和内存瓶颈问题，大多数最先进的方法使用了一个基于多实例学习（MIL）的三阶段弱监督流程：\n",
        "1. 在单个放大倍率（\"zoom\"）下对组织进行补丁化，例如20倍放大倍率。\n",
        "\n",
        "2. 对补丁级特征进行提取，以构建一组补丁嵌入（将补丁压缩100~500倍）。\n",
        "\n",
        "3. 对嵌入进行全局汇集，以构建一个幻灯片级别的表示，用于使用幻灯片级别标签（例如亚型、等级、阶段、生存、来源）的弱监督。\n",
        "\n",
        "**笔记本目标:**\n",
        "以下教程旨在区分肺腺癌（LUAD，所有肺癌的40%）与肺鳞状细胞癌（LUSC，所有肺癌的30%）（参见[Lu等人，Nature BME 2021](https://www.nature.com/articles/s41551-020-00682-w)和代码库[CLAM](https://github.com/mahmoodlab/CLAM)）。具体来说，我们将：\n",
        "- 训练和评估一个名为`AverageMIL`的\"朴素\" MIL 算法，该算法取补丁嵌入的平均值（作为全局汇集算子）。\n",
        "\n",
        "- 实现一个更复杂的算法，称为基于注意力的多实例学习(`ABMIL`)，该算法学习注意力权重来计算补丁嵌入的加权平均值。\n",
        "\n",
        "- 比较和对比 `AverageMIL` 和 `ABMIL`，讨论哪种算法更好，并讨论潜在限制。\n",
        "\n",
        "**关于这个笔记本**:\n",
        "- 模型实现和训练直接改编自[CLAM](https://github.com/mahmoodlab/CLAM)。CLAM包含许多额外功能（例如 - 允许用户设置优化器、模型类型、日志信息和其他超参数），但由于教学目的，这里简化了这个笔记本的运行。要使用所有功能，请参阅CLAM。\n",
        "\n",
        "- 尽管这个笔记本是基于CLAM构建的，但您将要实现的感兴趣的方法不是CLAM，而是来自[Ilse等人，ICML 2018](https://arxiv.org/abs/1802.04712)的另一种方法，称为ABMIL。\n",
        "\n",
        "- 虽然预提取特征是使用CLAM代码库生成的，但编码器不是在 ImageNet 上预训练的截断 ResNet-50（维度 1024）在20倍分辨率下。相反，我们使用了一个更小的CNN编码器（维度 320）在10倍分辨率下提取特征，这将数据集的大小从约11 GB 缩小到约3.96 GB 的存储空间（**在下面的单元格中提供了预提取特征的下载链接**）。此外，为了可重现性设置了一个torch.seed（所有输出应该是确定性的）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7ed950",
      "metadata": {
        "id": "8f7ed950"
      },
      "source": [
        "### Colab 安装、数据下载和依赖项\n",
        "\n",
        "- 获取预定义的 tcga-luad 和 tcga-lusc 的临床元数据 csv 文件，带有预先定义的训练/验证/测试拆分\n",
        "- 获取 tcga-luad 和 tcga-lusc 诊断 WSI 的预提取特征（共计 1043 个 WSI，大小约为 3.96 GB，下载时间约为 67 秒）\n",
        "\n",
        "或者，您可以直接从 Dropbox 下载数据到您的本地计算机，并在本地运行此 Colab 笔记本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "h1fJd7bzLpZH",
      "metadata": {
        "id": "h1fJd7bzLpZH"
      },
      "outputs": [],
      "source": [
        "use_drive = False\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  !mkdir -p \"/content/drive/My Drive/ai4healthsummerschool/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "QKGr936RiVFS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKGr936RiVFS",
        "outputId": "02c12827-fced-4b80-d1fd-5ff1268ce768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-05 17:53:04--  https://www.dropbox.com/s/5wuvu791vwntg9o/tcga_lung_splits.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/5wuvu791vwntg9o/tcga_lung_splits.csv [following]\n",
            "--2023-07-05 17:53:04--  https://www.dropbox.com/s/raw/5wuvu791vwntg9o/tcga_lung_splits.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com/cd/0/inline/B_RI62q1I4vP4bVo7yBoP0o0nLUydn_k3nt5sJPm3p2prkcqhVBmRl_VY0lgTyiDQftl3nJuiQ634aBMTMeXHpwgK1S5-lNLTyiHfgyBRiCiYZJxAOMWbb0Ey7xSFm28sAbvx0tXuUgMV_5xNW3WWGxB_aVb5KTeYe7kQu4VIp8MCg/file# [following]\n",
            "--2023-07-05 17:53:04--  https://uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com/cd/0/inline/B_RI62q1I4vP4bVo7yBoP0o0nLUydn_k3nt5sJPm3p2prkcqhVBmRl_VY0lgTyiDQftl3nJuiQ634aBMTMeXHpwgK1S5-lNLTyiHfgyBRiCiYZJxAOMWbb0Ey7xSFm28sAbvx0tXuUgMV_5xNW3WWGxB_aVb5KTeYe7kQu4VIp8MCg/file\n",
            "Resolving uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com (uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com (uc2584411338e312d2dc01a897a7.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170283 (166K) [text/plain]\n",
            "Saving to: ‘tcga_lung_splits.csv’\n",
            "\n",
            "tcga_lung_splits.cs 100%[===================>] 166.29K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-05 17:53:05 (4.58 MB/s) - ‘tcga_lung_splits.csv’ saved [170283/170283]\n",
            "\n",
            "--2023-07-05 17:53:05--  https://www.dropbox.com/s/euepd2owxvuwr7v/feats_pt.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/euepd2owxvuwr7v/feats_pt.zip [following]\n",
            "--2023-07-05 17:53:05--  https://www.dropbox.com/s/raw/euepd2owxvuwr7v/feats_pt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com/cd/0/inline/B_RH2SZ-Ta92Wb-qrBDceeYXa-JB1eVjyWKjVeBNspWsJzpzE331m1Fg50O1tjhKdRvPNi-rY0WQRZRLPJGZ-972BLL1WAgvgaayRSOEmhtKCJckE7ep5Hilf393DzseUJZ8QZBF82mEjRu9Yc6PopDf6exI0kLJffoA63AVsw_7ZA/file# [following]\n",
            "--2023-07-05 17:53:06--  https://uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com/cd/0/inline/B_RH2SZ-Ta92Wb-qrBDceeYXa-JB1eVjyWKjVeBNspWsJzpzE331m1Fg50O1tjhKdRvPNi-rY0WQRZRLPJGZ-972BLL1WAgvgaayRSOEmhtKCJckE7ep5Hilf393DzseUJZ8QZBF82mEjRu9Yc6PopDf6exI0kLJffoA63AVsw_7ZA/file\n",
            "Resolving uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com (uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com (uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B_Tx61hoZe6I-K4BRWWflNltZKjWw5s2PfGB6DEjTpTHA25qZfXhWiDYX7RyiTvR8wnPI2WVgyOWHZ-WNNPpv0C8KoZ2K93ZrV8NQX_ASv09Apnn9BERSo9BnJfHwuSPZ2O5lMi396MkPxKGYxlYBa5LbMs9ob62nuyPzNFT15LnMvN4tFN0sDuAhexMObdwpBbSmaegpcre3nRvVYbkDK8IPPR2tovWHsMXz0mYBBHwYGfcNUVFZ18s-pUBvxNWeIR__AJ5PYZSPSih0R3g_nAZVb8JLUzwdAqRvxWt4vJO3gwMPGf9EOIwrnP8n9UbZa6pLqTN91bFmsPzMNAay_bUDTaJ14rENkTbjB7kDDRoYi4Y8bsohneP_C_d3AyIos7O38zSrm3oZWdx8g58tYEWI2VIp-82kozJhWiF32jXdQ/file [following]\n",
            "--2023-07-05 17:53:06--  https://uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com/cd/0/inline2/B_Tx61hoZe6I-K4BRWWflNltZKjWw5s2PfGB6DEjTpTHA25qZfXhWiDYX7RyiTvR8wnPI2WVgyOWHZ-WNNPpv0C8KoZ2K93ZrV8NQX_ASv09Apnn9BERSo9BnJfHwuSPZ2O5lMi396MkPxKGYxlYBa5LbMs9ob62nuyPzNFT15LnMvN4tFN0sDuAhexMObdwpBbSmaegpcre3nRvVYbkDK8IPPR2tovWHsMXz0mYBBHwYGfcNUVFZ18s-pUBvxNWeIR__AJ5PYZSPSih0R3g_nAZVb8JLUzwdAqRvxWt4vJO3gwMPGf9EOIwrnP8n9UbZa6pLqTN91bFmsPzMNAay_bUDTaJ14rENkTbjB7kDDRoYi4Y8bsohneP_C_d3AyIos7O38zSrm3oZWdx8g58tYEWI2VIp-82kozJhWiF32jXdQ/file\n",
            "Reusing existing connection to uc7c96ff4e834a7fbe7fe428b46e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3961539230 (3.7G) [application/zip]\n",
            "Saving to: ‘feats_pt.zip’\n",
            "\n",
            "feats_pt.zip        100%[===================>]   3.69G  89.3MB/s    in 42s     \n",
            "\n",
            "2023-07-05 17:53:48 (90.9 MB/s) - ‘feats_pt.zip’ saved [3961539230/3961539230]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# either download in colab (data will be deleted when re-starting) or mount your labdrive (preferred, but requires 4GB of storage)\n",
        "if use_drive:\n",
        "  !wget https://www.dropbox.com/s/5wuvu791vwntg9o/tcga_lung_splits.csv -P \"/content/drive/My Drive/ai4healthsummerschool\"\n",
        "  !wget https://www.dropbox.com/s/euepd2owxvuwr7v/feats_pt.zip\n",
        "  !unzip -q feats_pt.zip\n",
        "  !mv feats_pt \"/content/drive/My Drive/ai4healthsummerschool\"\n",
        "else:\n",
        "  !wget https://www.dropbox.com/s/5wuvu791vwntg9o/tcga_lung_splits.csv\n",
        "  !wget https://www.dropbox.com/s/euepd2owxvuwr7v/feats_pt.zip\n",
        "  !unzip -q feats_pt.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SEsPRU9wLnRl",
      "metadata": {
        "id": "SEsPRU9wLnRl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c5a99a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5a99a0",
        "outputId": "14a5a6f3-9f29-43d1-800c-e77fac597f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53517df",
      "metadata": {
        "id": "e53517df"
      },
      "source": [
        "### WSI data preprocessing for histology slides in the TCGA-Lung cohort\n",
        "\n",
        "\n",
        "![](https://user-images.githubusercontent.com/10300839/232984010-7f8e3a6f-e0c5-4847-8d0f-747460055528.png)\n",
        "\n",
        "要处理WSIs，通常使用诸如[CLAM](https://github.com/mahmoodlab/CLAM)之类的工具进行组织补丁和非重叠补丁特征提取。尽管易于使用，但使用CLAM进行特征处理需要下载千亿像素的WSIs（TCGA-LUAD和TCGA-LUSC中超过1000个WSIS），超过100GB的存储空间。为了缓解这个问题，这个问题集提供了预提取的特征（通过CLAM处理，但使用了一个大小远远小于$D=320$的视觉编码器）。然而，为了仍然说明CLAM预处理的工作原理，下面的单元格描述了WSIs如何被构造为`[M x D]`维的补丁嵌入包，其中`M`是组织补丁的数量，`D`是您编码器的隐藏维度大小。再次强调，如果您有兴趣重新生成这些特征，请使用[CLAM](https://github.com/mahmoodlab/CLAM)。\n",
        "\n",
        "**注意：** 不需要运行此单元格来训练最终模型！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a94a7d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a94a7d83",
        "outputId": "c5186d2e-4342-4f45-8b6b-3fbd3d601e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WSI Shape: torch.Size([2, 3, 256, 256])\n",
            "Probability Scores for ImageNet: torch.Size([1, 1000])\n",
            "Feature Embedding Shape: torch.Size([1, 576])\n",
            "Bag Shape (2, 576)\n"
          ]
        }
      ],
      "source": [
        "# 假设我们有一个\"M个[256 x 256 x 3]图像补丁的集合（M = 512），这些补丁是从WSI中的非重叠补丁中获取的。\n",
        "M = 2\n",
        "X = torch.randn(M, 3, 256, 256) # 排列格式为（Batch，Channel，Width，Height）或简写为（B，C，W，H）\n",
        "print(\"WSI形状:\", X.shape)\n",
        "\n",
        "# 例如，我们将使用CNN模型（在ImageNet上预训练）作为我们的视觉编码器，用于从每个补丁中预提取“压缩”的表示。\n",
        "cnn = torchvision.models.mobilenet_v3_small()\n",
        "cnn.eval()\n",
        "\n",
        "# 由于这个模型来自torchvision，并在ImageNet上训练，模型的输出是ImageNet类的概率分数（总共1000个类）。\n",
        "# 要从每个补丁中提取有用的特征，我们必须使用CNN的倒数第二层（或倒数第二层）输出，然后将其馈送到线性层中。\n",
        "print(\"ImageNet的概率分数:\", cnn.forward(X[:1]).shape)\n",
        "\n",
        "# 为了提取倒数第二层的特征，我们可以定义一个新函数，该函数返回特征，而不是将其馈送到模型内部的分类器层中。\n",
        "# 再次强调，我们希望使用在ImageNet上预训练的特征，但不需要“ImageNet”类别的分类分数！\n",
        "# 请参见下面的文档，了解MobileNetV3中forward pass的工作原理。\n",
        "# https://pytorch.org/vision/main/_modules/torchvision/models/mobilenetv3.html#mobilenet_v3_small\n",
        "encoder = lambda x: torch.flatten(cnn.avgpool(cnn.features(x)), 1)\n",
        "print(\"特征嵌入形状:\", encoder(X[:1]).shape)\n",
        "\n",
        "# 现在，我们可以使用我们的编码器来提取每个补丁的特征。\n",
        "# 通常，WSI中非重叠补丁的数量约为15000个。因此，我们经常需要以小批量方式提取补丁特征。\n",
        "batch_size = 32\n",
        "H = []\n",
        "for bag_idx in range(0, M, batch_size):\n",
        "    H.append(encoder(X[bag_idx:(bag_idx+batch_size)]).cpu().detach().numpy())\n",
        "print(\"Bag形状\", np.vstack(H).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ohfe2sWOUD5q",
      "metadata": {
        "id": "Ohfe2sWOUD5q"
      },
      "source": [
        "### 数据探索\n",
        "\n",
        "**注意：** 不需要运行此单元格来训练最终模型！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "R1rXRyqPRaKk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "R1rXRyqPRaKk",
        "outputId": "0179d85b-291a-46df-dd0e-78b66fad2d33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4a4782bd-0a61-4e44-b4cf-8c28458fc9bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_id</th>\n",
              "      <th>slide_id</th>\n",
              "      <th>tumor_type</th>\n",
              "      <th>OncoTreeSiteCode</th>\n",
              "      <th>main_cancer_type</th>\n",
              "      <th>sex</th>\n",
              "      <th>project_id</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>OncoTreeCode</th>\n",
              "      <th>OncoTreeCode_Binarized</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-73-4676</td>\n",
              "      <td>TCGA-73-4676-01Z-00-DX1.4d781bbc-a45e-4f9d-b6b...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-MP-A4T6</td>\n",
              "      <td>TCGA-MP-A4T6-01Z-00-DX1.085C4F5A-DB1B-434A-9D6...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-78-7167</td>\n",
              "      <td>TCGA-78-7167-01Z-00-DX1.f79e1a9b-a3eb-4c91-a1f...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-L9-A444</td>\n",
              "      <td>TCGA-L9-A444-01Z-00-DX1.88CF6F01-0C1F-4572-81E...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-55-8097</td>\n",
              "      <td>TCGA-55-8097-01Z-00-DX1.2f847b65-a5dc-41be-9dd...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>TCGA-21-A5DI</td>\n",
              "      <td>TCGA-21-A5DI-01Z-00-DX1.E9123261-ADE7-468C-9E9...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUSC</td>\n",
              "      <td>Lung Squamous Cell Carcinoma</td>\n",
              "      <td>LUSC</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1039</th>\n",
              "      <td>TCGA-77-7465</td>\n",
              "      <td>TCGA-77-7465-01Z-00-DX1.25e4b0b4-4948-432f-801...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUSC</td>\n",
              "      <td>Lung Squamous Cell Carcinoma</td>\n",
              "      <td>LUSC</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040</th>\n",
              "      <td>TCGA-34-8454</td>\n",
              "      <td>TCGA-34-8454-01Z-00-DX1.A2308ED3-E430-4448-853...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUSC</td>\n",
              "      <td>Lung Squamous Cell Carcinoma</td>\n",
              "      <td>LUSC</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1041</th>\n",
              "      <td>TCGA-77-7138</td>\n",
              "      <td>TCGA-77-7138-01Z-00-DX1.8c912762-0829-4692-92a...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUSC</td>\n",
              "      <td>Lung Squamous Cell Carcinoma</td>\n",
              "      <td>LUSC</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042</th>\n",
              "      <td>TCGA-77-8131</td>\n",
              "      <td>TCGA-77-8131-01Z-00-DX1.dcb8e2c7-0d2f-4b38-9db...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUSC</td>\n",
              "      <td>Lung Squamous Cell Carcinoma</td>\n",
              "      <td>LUSC</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1043 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a4782bd-0a61-4e44-b4cf-8c28458fc9bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a4782bd-0a61-4e44-b4cf-8c28458fc9bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a4782bd-0a61-4e44-b4cf-8c28458fc9bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           case_id                                           slide_id  \\\n",
              "0     TCGA-73-4676  TCGA-73-4676-01Z-00-DX1.4d781bbc-a45e-4f9d-b6b...   \n",
              "1     TCGA-MP-A4T6  TCGA-MP-A4T6-01Z-00-DX1.085C4F5A-DB1B-434A-9D6...   \n",
              "2     TCGA-78-7167  TCGA-78-7167-01Z-00-DX1.f79e1a9b-a3eb-4c91-a1f...   \n",
              "3     TCGA-L9-A444  TCGA-L9-A444-01Z-00-DX1.88CF6F01-0C1F-4572-81E...   \n",
              "4     TCGA-55-8097  TCGA-55-8097-01Z-00-DX1.2f847b65-a5dc-41be-9dd...   \n",
              "...            ...                                                ...   \n",
              "1038  TCGA-21-A5DI  TCGA-21-A5DI-01Z-00-DX1.E9123261-ADE7-468C-9E9...   \n",
              "1039  TCGA-77-7465  TCGA-77-7465-01Z-00-DX1.25e4b0b4-4948-432f-801...   \n",
              "1040  TCGA-34-8454  TCGA-34-8454-01Z-00-DX1.A2308ED3-E430-4448-853...   \n",
              "1041  TCGA-77-7138  TCGA-77-7138-01Z-00-DX1.8c912762-0829-4692-92a...   \n",
              "1042  TCGA-77-8131  TCGA-77-8131-01Z-00-DX1.dcb8e2c7-0d2f-4b38-9db...   \n",
              "\n",
              "     tumor_type OncoTreeSiteCode            main_cancer_type sex project_id  \\\n",
              "0       Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "1       Primary             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "2       Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "3       Primary             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "4       Primary             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "...         ...              ...                         ...  ..        ...   \n",
              "1038    Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUSC   \n",
              "1039    Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUSC   \n",
              "1040    Primary             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUSC   \n",
              "1041    Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUSC   \n",
              "1042    Primary             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUSC   \n",
              "\n",
              "                         Diagnosis OncoTreeCode  OncoTreeCode_Binarized  split  \n",
              "0              Lung Adenocarcinoma         LUAD                       0  train  \n",
              "1              Lung Adenocarcinoma         LUAD                       0  train  \n",
              "2              Lung Adenocarcinoma         LUAD                       0  train  \n",
              "3              Lung Adenocarcinoma         LUAD                       0  train  \n",
              "4              Lung Adenocarcinoma         LUAD                       0  train  \n",
              "...                            ...          ...                     ...    ...  \n",
              "1038  Lung Squamous Cell Carcinoma         LUSC                       1   test  \n",
              "1039  Lung Squamous Cell Carcinoma         LUSC                       1   test  \n",
              "1040  Lung Squamous Cell Carcinoma         LUSC                       1   test  \n",
              "1041  Lung Squamous Cell Carcinoma         LUSC                       1   test  \n",
              "1042  Lung Squamous Cell Carcinoma         LUSC                       1   test  \n",
              "\n",
              "[1043 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "split  OncoTreeCode\n",
              "train  LUAD            433\n",
              "       LUSC            415\n",
              "test   LUAD             49\n",
              "       LUSC             49\n",
              "val    LUAD             49\n",
              "       LUSC             48\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example filenames for extracted features: ['TCGA-75-6205-01Z-00-DX1.B75BC6BA-5196-4F62-BDA2-3F1D320ABD7C.pt', 'TCGA-77-8153-01Z-00-DX1.E8E40968-E7AD-4EA2-A832-8EC04D5CB7A1.pt', 'TCGA-56-8083-01Z-00-DX1.140c8d5b-f660-4fef-b8da-6bb2c119c021.pt', 'TCGA-73-A9RS-01Z-00-DX1.EDCEFE41-61E2-48C9-B8D5-28B55372E0CA.pt', 'TCGA-56-8201-01Z-00-DX1.883903fb-d70d-4c72-be76-6788b1bc3b35.pt']\n",
            "Overlap of extracted feature filenames + slide_id column: 1043\n",
            "Mean Bag Size: 3259.9090038314175\n",
            "Std Bag Size: 2133.97437395412\n"
          ]
        }
      ],
      "source": [
        "# 我们下载特征和标签csv的位置\n",
        "use_drive = False\n",
        "if use_drive:\n",
        "  feats_dirpath, csv_fpath = '/content/drive/My Drive/ai4healthsummerschool/feats_pt/', '/content/drive/My Drive/ai4healthsummerschool/tcga_lung_splits.csv'\n",
        "else:\n",
        "  feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
        "\n",
        "# 标签csv匹配case_id（患者）、slide_id（WSI图像文件名）和诊断（LUAD vs LUSC）\n",
        "# 以及预定义的拆分（train / val / test）\n",
        "df = pd.read_csv(csv_fpath)\n",
        "display(df)\n",
        "display(df[['split', 'OncoTreeCode']].value_counts())\n",
        "\n",
        "# 提取的特征文件名+slide_id列匹配\n",
        "feats_pt_fnames = pd.Series(os.listdir(feats_dirpath))\n",
        "print(\"提取的特征的示例文件名:\", list(feats_pt_fnames[:5]))\n",
        "print(\"提取的特征文件名与slide_id列的重叠:\",\n",
        "      len(set(df['slide_id']).intersection(set(feats_pt_fnames.str[:-3]))))\n",
        "\n",
        "# 每个包的大小的统计信息\n",
        "bag_sizes = []\n",
        "for e in os.scandir(feats_dirpath):\n",
        "    feats_pt = torch.load(e.path)    # [M x d]-dim tensor\n",
        "    bag_sizes.append(feats_pt.shape[0])\n",
        "print('平均包大小:', np.mean(bag_sizes))\n",
        "print('包大小标准差:', np.std(bag_sizes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cddf94",
      "metadata": {
        "id": "50cddf94"
      },
      "source": [
        "### 模型 1: AverageMIL\n",
        "\n",
        "实现了一个最简单的训练设置，通过 `AverageMIL` 在 LUAD vs. LUSC 亚型上进行弱监督学习，使用了来自癌症基因组图谱（The Cancer Genome Atlas）的1043个诊断 H\\&E 组织切片（特征已经预先提取并从安装中下载，所有案例和切片ID的临床元数据也已经下载）。\n",
        "\n",
        "您可以在 Google Colab 笔记本中运行这些单元格，并查看该算法在 20 个周期内的性能如何。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bd9a3278",
      "metadata": {
        "id": "bd9a3278"
      },
      "outputs": [],
      "source": [
        "class AverageMIL(nn.Module):\n",
        "    def __init__(self, input_dim=320, hidden_dim=64, dropout=0.25, n_classes=2):\n",
        "        r\"\"\"\n",
        "        AverageMIL, 一个简单的MIL算法，将所有补丁特征进行平均池化。\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): 输入特征维度。\n",
        "            hidden_dim (int): 隐藏层维度。\n",
        "            dropout (float): Dropout概率。\n",
        "            n_classes (int): 类别数。\n",
        "        \"\"\"\n",
        "        super(AverageMIL, self).__init__()\n",
        "        self.inst_level_fc = nn.Sequential(*[nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]) # \"实例级\"对每个嵌入应用的全连接层\n",
        "        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)                                            # 包级别分类器\n",
        "\n",
        "    def forward(self, H):\n",
        "        r\"\"\"\n",
        "        接受一个[M x D]维的补丁特征包（表示一个WSI），并输出：1) 用于分类的logits，2) 未归一化的注意力分数。\n",
        "\n",
        "        Args:\n",
        "            H (torch.Tensor): [M x D]维的补丁特征包（表示一个WSI）\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): [1 x n_classes]维张量，用于分类任务的未归一化logits。\n",
        "            None (不返回注意力分数)\n",
        "        \"\"\"\n",
        "        H = self.inst_level_fc(H)                   # 1. 预处理每个“实例级”嵌入，使其变为“隐藏维”维度大小\n",
        "        z = H.mean(dim=0).unsqueeze(dim=0)          # 2. 补丁嵌入的平均值\n",
        "        logits = self.bag_level_classifier(z)       # 3. 包级别分类器\n",
        "        return logits, None\n",
        "\n",
        "\n",
        "class MILDataset(torch.utils.data.dataset.Dataset):\n",
        "    r\"\"\"\n",
        "    torch.utils.data.dataset.Dataset对象，从CSV中加载每个WSI的预提取特征。\n",
        "\n",
        "    Args:\n",
        "        feats_dirpath (str): 预提取补丁特征的路径（假设这些特征保存为带有对应slide_id的文件名的*.pt对象）\n",
        "        csv_fpath (str): 包含以下内容的CSV文件路径：1) Case ID, 2) Slide ID, 3) 拆分信息（train / val / test），以及4) 用于分类的标签列。\n",
        "        which_split (str): 用于子集化CSV的拆分（选择：['train', 'val', 'test']）\n",
        "        n_classes (int): 类别数（默认为2，用于LUAD vs LUSC亚型）\n",
        "    \"\"\"\n",
        "    def __init__(self, feats_dirpath='./', csv_fpath='./tcga_lung_splits.csv', which_split='train', which_labelcol='OncoTreeCode_Binarized'):\n",
        "        self.feats_dirpath, self.csv, self.which_labelcol = feats_dirpath, pd.read_csv(csv_fpath), which_labelcol\n",
        "        self.csv_split = self.csv[self.csv['split']==which_split]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        features = torch.load(os.path.join(self.feats_dirpath, self.csv_split.iloc[index]['slide_id']+'.pt'))\n",
        "        label = self.csv_split.iloc[index][self.which_labelcol]\n",
        "        return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv_split.shape[0]\n",
        "\n",
        "\n",
        "def traineval_epoch(epoch, model, loader, optimizer=None, loss_fn=nn.CrossEntropyLoss(), split='train', device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), verbose=1, print_every=300):\n",
        "    r\"\"\"\n",
        "    执行一轮训练/评估的函数，使用torch.nn模型在torch.utils.data.DataLoader对象上。\n",
        "    通常，这些函数分别为训练和验证定义，但为了节省行数，我们将它们合并在一起。\n",
        "\n",
        "    Args:\n",
        "        epoch (int): 当前训练/评估的轮次（用于记录）。\n",
        "        model (torch.nn): 用于处理补丁特征包的MIL模型。\n",
        "        loader (torch.utils.data.DataLoader): 获取每个WSI的补丁特征包的对象。\n",
        "        loss_fn (torch.nn): 损失函数。\n",
        "        split (str): 使用的拆分，用于设置模型 + 计算损失 + 计算梯度。\n",
        "        device (torch): 表示将分配到的设备上的torch.Tensor的对象。\n",
        "        verbose (int): 是否打印摘要epoch结果（verbose >=1）和迭代信息（verbose >=2）。\n",
        "        print_every (int): 每多少个批次迭代打印一次\n",
        "\n",
        "    Returns:\n",
        "        log_dict (dict): 用于记录训练/验证/测试拆分的损失和性能的字典。\n",
        "    \"\"\"\n",
        "    model.train() if (split == 'train') else model.eval()       # 根据拆分设置模型是否用于训练或评估\n",
        "    total_loss, Y_probs, labels = 0.0, [], []                   # 跟踪损失 + logits/标签用于性能指标\n",
        "    for batch_idx, (X_bag, label) in enumerate(loader):\n",
        "        # 由于我们假设批量大小== 1，我们希望阻止torch将我们的补丁特征包作为[1 x M x D] torch张量进行整理。\n",
        "        X_bag, label = X_bag[0].to(device), label.to(device)\n",
        "\n",
        "        if (split == 'train'):\n",
        "            logits, A_norm = model(X_bag)\n",
        "            loss = loss_fn(logits, label)\n",
        "            loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        else:\n",
        "            with torch.no_grad(): logits, A_norm = model(X_bag)\n",
        "            loss = loss_fn(logits, label)\n",
        "\n",
        "        # 跟踪总损失、logits和当前进度\n",
        "        total_loss += loss.item()\n",
        "        Y_probs.append(torch.softmax(logits, dim=-1).cpu().detach().numpy())\n",
        "        labels.append(label.cpu().detach().numpy())\n",
        "        if ((batch_idx + 1) % print_every == 0) and (verbose >= 2):\n",
        "            print(f'Epoch {epoch}:\\t Batch {batch_idx}\\t Avg Loss: {total_loss / (batch_idx+1):.04f}\\t Label: {label.item()}\\t Bag Size: {X_bag.shape[0]}')\n",
        "\n",
        "    # 从保存的logits /标签计算平衡准确度和AUC-ROC\n",
        "    Y_probs, labels = np.vstack(Y_probs), np.concatenate(labels)\n",
        "    log_dict = {f'{split} loss': total_loss/len(loader),\n",
        "                f'{split} acc': sklearn.metrics.balanced_accuracy_score(labels, Y_probs.argmax(axis=1)),\n",
        "                f'{split} auc': sklearn.metrics.roc_auc_score(labels, Y_probs[:, 1])}\n",
        "\n",
        "    # 打印epoch结束信息\n",
        "    if (verbose >= 1):\n",
        "        print(f'### ({split.capitalize()} Summary) ###')\n",
        "        print(f'Epoch {epoch}:\\t' + f'\\t'.join([f'{k.capitalize().rjust(10)}: {log_dict[k]:.04f}' for k,v in log_dict.items()]))\n",
        "    return log_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71e79d24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e79d24",
        "outputId": "486e45ad-ce69-451a-f2e4-60a923ff7cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:\t Batch 199\t Avg Loss: 0.7137\t Label: 1\t Bag Size: 957\n",
            "Epoch 0:\t Batch 399\t Avg Loss: 0.7084\t Label: 1\t Bag Size: 1131\n",
            "Epoch 0:\t Batch 599\t Avg Loss: 0.7093\t Label: 1\t Bag Size: 1342\n",
            "Epoch 0:\t Batch 799\t Avg Loss: 0.7075\t Label: 1\t Bag Size: 2319\n",
            "### (Train Summary) ###\n",
            "Epoch 0:\tTrain loss: 0.7067\t Train acc: 0.5178\t Train auc: 0.5205\n",
            "### (Val Summary) ###\n",
            "Epoch 0:\t  Val loss: 0.6804\t   Val acc: 0.5627\t   Val auc: 0.6947\n",
            "\n",
            "Epoch 1:\t Batch 199\t Avg Loss: 0.6892\t Label: 1\t Bag Size: 2830\n",
            "Epoch 1:\t Batch 399\t Avg Loss: 0.6933\t Label: 0\t Bag Size: 4905\n",
            "Epoch 1:\t Batch 599\t Avg Loss: 0.6951\t Label: 0\t Bag Size: 5624\n",
            "Epoch 1:\t Batch 799\t Avg Loss: 0.6919\t Label: 0\t Bag Size: 1298\n",
            "### (Train Summary) ###\n",
            "Epoch 1:\tTrain loss: 0.6945\t Train acc: 0.5530\t Train auc: 0.5598\n",
            "### (Val Summary) ###\n",
            "Epoch 1:\t  Val loss: 0.6736\t   Val acc: 0.5525\t   Val auc: 0.7105\n",
            "\n",
            "Epoch 2:\t Batch 199\t Avg Loss: 0.6909\t Label: 0\t Bag Size: 3509\n",
            "Epoch 2:\t Batch 399\t Avg Loss: 0.6808\t Label: 0\t Bag Size: 6975\n",
            "Epoch 2:\t Batch 599\t Avg Loss: 0.6799\t Label: 0\t Bag Size: 1555\n",
            "Epoch 2:\t Batch 799\t Avg Loss: 0.6784\t Label: 0\t Bag Size: 4302\n",
            "### (Train Summary) ###\n",
            "Epoch 2:\tTrain loss: 0.6773\t Train acc: 0.5668\t Train auc: 0.6022\n",
            "### (Val Summary) ###\n",
            "Epoch 2:\t  Val loss: 0.6703\t   Val acc: 0.5215\t   Val auc: 0.7105\n",
            "\n",
            "Epoch 3:\t Batch 199\t Avg Loss: 0.6628\t Label: 0\t Bag Size: 6237\n",
            "Epoch 3:\t Batch 399\t Avg Loss: 0.6645\t Label: 0\t Bag Size: 6577\n",
            "Epoch 3:\t Batch 599\t Avg Loss: 0.6678\t Label: 1\t Bag Size: 223\n",
            "Epoch 3:\t Batch 799\t Avg Loss: 0.6640\t Label: 0\t Bag Size: 3368\n",
            "### (Train Summary) ###\n",
            "Epoch 3:\tTrain loss: 0.6653\t Train acc: 0.5916\t Train auc: 0.6377\n",
            "### (Val Summary) ###\n",
            "Epoch 3:\t  Val loss: 0.6588\t   Val acc: 0.6507\t   Val auc: 0.7164\n",
            "\n",
            "Epoch 4:\t Batch 199\t Avg Loss: 0.6642\t Label: 1\t Bag Size: 1563\n",
            "Epoch 4:\t Batch 399\t Avg Loss: 0.6751\t Label: 0\t Bag Size: 1512\n",
            "Epoch 4:\t Batch 599\t Avg Loss: 0.6637\t Label: 1\t Bag Size: 8248\n",
            "Epoch 4:\t Batch 799\t Avg Loss: 0.6586\t Label: 1\t Bag Size: 933\n",
            "### (Train Summary) ###\n",
            "Epoch 4:\tTrain loss: 0.6589\t Train acc: 0.5951\t Train auc: 0.6462\n",
            "### (Val Summary) ###\n",
            "Epoch 4:\t  Val loss: 0.7084\t   Val acc: 0.5000\t   Val auc: 0.7092\n",
            "\n",
            "Epoch 5:\t Batch 199\t Avg Loss: 0.6623\t Label: 0\t Bag Size: 377\n",
            "Epoch 5:\t Batch 399\t Avg Loss: 0.6632\t Label: 0\t Bag Size: 1856\n",
            "Epoch 5:\t Batch 599\t Avg Loss: 0.6511\t Label: 1\t Bag Size: 3239\n",
            "Epoch 5:\t Batch 799\t Avg Loss: 0.6467\t Label: 0\t Bag Size: 356\n",
            "### (Train Summary) ###\n",
            "Epoch 5:\tTrain loss: 0.6434\t Train acc: 0.6376\t Train auc: 0.6816\n",
            "### (Val Summary) ###\n",
            "Epoch 5:\t  Val loss: 0.6920\t   Val acc: 0.5000\t   Val auc: 0.7083\n",
            "\n",
            "Epoch 6:\t Batch 199\t Avg Loss: 0.6496\t Label: 1\t Bag Size: 4575\n",
            "Epoch 6:\t Batch 399\t Avg Loss: 0.6349\t Label: 1\t Bag Size: 4743\n",
            "Epoch 6:\t Batch 599\t Avg Loss: 0.6317\t Label: 0\t Bag Size: 4420\n",
            "Epoch 6:\t Batch 799\t Avg Loss: 0.6311\t Label: 1\t Bag Size: 4480\n",
            "### (Train Summary) ###\n",
            "Epoch 6:\tTrain loss: 0.6323\t Train acc: 0.6517\t Train auc: 0.7069\n",
            "### (Val Summary) ###\n",
            "Epoch 6:\t  Val loss: 0.7329\t   Val acc: 0.5000\t   Val auc: 0.7083\n",
            "\n",
            "Epoch 7:\t Batch 199\t Avg Loss: 0.6297\t Label: 0\t Bag Size: 4443\n",
            "Epoch 7:\t Batch 399\t Avg Loss: 0.6245\t Label: 1\t Bag Size: 1261\n",
            "Epoch 7:\t Batch 599\t Avg Loss: 0.6233\t Label: 1\t Bag Size: 4893\n",
            "Epoch 7:\t Batch 799\t Avg Loss: 0.6257\t Label: 1\t Bag Size: 5530\n",
            "### (Train Summary) ###\n",
            "Epoch 7:\tTrain loss: 0.6244\t Train acc: 0.6617\t Train auc: 0.7160\n",
            "### (Val Summary) ###\n",
            "Epoch 7:\t  Val loss: 0.6490\t   Val acc: 0.6471\t   Val auc: 0.7117\n",
            "\n",
            "Epoch 8:\t Batch 199\t Avg Loss: 0.6155\t Label: 1\t Bag Size: 5553\n",
            "Epoch 8:\t Batch 399\t Avg Loss: 0.6293\t Label: 0\t Bag Size: 8315\n",
            "Epoch 8:\t Batch 599\t Avg Loss: 0.6184\t Label: 0\t Bag Size: 4905\n",
            "Epoch 8:\t Batch 799\t Avg Loss: 0.6141\t Label: 1\t Bag Size: 3630\n",
            "### (Train Summary) ###\n",
            "Epoch 8:\tTrain loss: 0.6119\t Train acc: 0.6746\t Train auc: 0.7349\n",
            "### (Val Summary) ###\n",
            "Epoch 8:\t  Val loss: 0.6426\t   Val acc: 0.6477\t   Val auc: 0.7122\n",
            "\n",
            "Epoch 9:\t Batch 199\t Avg Loss: 0.6157\t Label: 0\t Bag Size: 2482\n",
            "Epoch 9:\t Batch 399\t Avg Loss: 0.6002\t Label: 0\t Bag Size: 1856\n",
            "Epoch 9:\t Batch 599\t Avg Loss: 0.6062\t Label: 0\t Bag Size: 384\n",
            "Epoch 9:\t Batch 799\t Avg Loss: 0.6094\t Label: 0\t Bag Size: 579\n",
            "### (Train Summary) ###\n",
            "Epoch 9:\tTrain loss: 0.6099\t Train acc: 0.6767\t Train auc: 0.7334\n",
            "### (Val Summary) ###\n",
            "Epoch 9:\t  Val loss: 0.6621\t   Val acc: 0.6112\t   Val auc: 0.7151\n",
            "\n",
            "Epoch 10:\t Batch 199\t Avg Loss: 0.5707\t Label: 1\t Bag Size: 3912\n",
            "Epoch 10:\t Batch 399\t Avg Loss: 0.6012\t Label: 0\t Bag Size: 6484\n",
            "Epoch 10:\t Batch 599\t Avg Loss: 0.6039\t Label: 1\t Bag Size: 3609\n",
            "Epoch 10:\t Batch 799\t Avg Loss: 0.5971\t Label: 0\t Bag Size: 345\n",
            "### (Train Summary) ###\n",
            "Epoch 10:\tTrain loss: 0.5959\t Train acc: 0.6872\t Train auc: 0.7599\n",
            "### (Val Summary) ###\n",
            "Epoch 10:\t  Val loss: 0.6437\t   Val acc: 0.6577\t   Val auc: 0.7164\n",
            "\n",
            "Epoch 11:\t Batch 199\t Avg Loss: 0.5891\t Label: 0\t Bag Size: 1329\n",
            "Epoch 11:\t Batch 399\t Avg Loss: 0.6049\t Label: 0\t Bag Size: 3423\n",
            "Epoch 11:\t Batch 599\t Avg Loss: 0.5980\t Label: 1\t Bag Size: 5204\n",
            "Epoch 11:\t Batch 799\t Avg Loss: 0.5910\t Label: 1\t Bag Size: 1342\n",
            "### (Train Summary) ###\n",
            "Epoch 11:\tTrain loss: 0.5950\t Train acc: 0.6931\t Train auc: 0.7554\n",
            "### (Val Summary) ###\n",
            "Epoch 11:\t  Val loss: 0.6243\t   Val acc: 0.6913\t   Val auc: 0.7151\n",
            "Resetting early-stopping counter: inf -> 0.6243...\n",
            "\n",
            "Epoch 12:\t Batch 199\t Avg Loss: 0.5887\t Label: 1\t Bag Size: 5400\n",
            "Epoch 12:\t Batch 399\t Avg Loss: 0.5753\t Label: 0\t Bag Size: 6074\n",
            "Epoch 12:\t Batch 599\t Avg Loss: 0.5858\t Label: 0\t Bag Size: 4085\n",
            "Epoch 12:\t Batch 799\t Avg Loss: 0.5852\t Label: 0\t Bag Size: 2415\n",
            "### (Train Summary) ###\n",
            "Epoch 12:\tTrain loss: 0.5874\t Train acc: 0.7049\t Train auc: 0.7621\n",
            "### (Val Summary) ###\n",
            "Epoch 12:\t  Val loss: 0.6305\t   Val acc: 0.6482\t   Val auc: 0.7194\n",
            "Early-stopping counter updating: 0/5 -> 1/5...\n",
            "\n",
            "Epoch 13:\t Batch 199\t Avg Loss: 0.5654\t Label: 0\t Bag Size: 4486\n",
            "Epoch 13:\t Batch 399\t Avg Loss: 0.5794\t Label: 0\t Bag Size: 464\n",
            "Epoch 13:\t Batch 599\t Avg Loss: 0.5889\t Label: 0\t Bag Size: 8841\n",
            "Epoch 13:\t Batch 799\t Avg Loss: 0.5794\t Label: 0\t Bag Size: 1334\n",
            "### (Train Summary) ###\n",
            "Epoch 13:\tTrain loss: 0.5819\t Train acc: 0.6895\t Train auc: 0.7633\n",
            "### (Val Summary) ###\n",
            "Epoch 13:\t  Val loss: 0.6198\t   Val acc: 0.6913\t   Val auc: 0.7224\n",
            "Resetting early-stopping counter: 0.6243 -> 0.6198...\n",
            "\n",
            "Epoch 14:\t Batch 199\t Avg Loss: 0.5543\t Label: 0\t Bag Size: 2093\n",
            "Epoch 14:\t Batch 399\t Avg Loss: 0.5699\t Label: 0\t Bag Size: 9103\n",
            "Epoch 14:\t Batch 599\t Avg Loss: 0.5784\t Label: 1\t Bag Size: 2906\n",
            "Epoch 14:\t Batch 799\t Avg Loss: 0.5836\t Label: 1\t Bag Size: 1805\n",
            "### (Train Summary) ###\n",
            "Epoch 14:\tTrain loss: 0.5861\t Train acc: 0.6993\t Train auc: 0.7626\n",
            "### (Val Summary) ###\n",
            "Epoch 14:\t  Val loss: 0.6432\t   Val acc: 0.6412\t   Val auc: 0.7245\n",
            "Early-stopping counter updating: 0/5 -> 1/5...\n",
            "\n",
            "Epoch 15:\t Batch 199\t Avg Loss: 0.5710\t Label: 1\t Bag Size: 711\n",
            "Epoch 15:\t Batch 399\t Avg Loss: 0.5635\t Label: 1\t Bag Size: 980\n",
            "Epoch 15:\t Batch 599\t Avg Loss: 0.5850\t Label: 0\t Bag Size: 3167\n",
            "Epoch 15:\t Batch 799\t Avg Loss: 0.5800\t Label: 1\t Bag Size: 5346\n",
            "### (Train Summary) ###\n",
            "Epoch 15:\tTrain loss: 0.5789\t Train acc: 0.7088\t Train auc: 0.7732\n",
            "### (Val Summary) ###\n",
            "Epoch 15:\t  Val loss: 0.6718\t   Val acc: 0.5946\t   Val auc: 0.7287\n",
            "Early-stopping counter updating: 1/5 -> 2/5...\n",
            "\n",
            "Epoch 16:\t Batch 199\t Avg Loss: 0.5943\t Label: 0\t Bag Size: 4156\n",
            "Epoch 16:\t Batch 399\t Avg Loss: 0.5866\t Label: 1\t Bag Size: 2068\n",
            "Epoch 16:\t Batch 599\t Avg Loss: 0.5768\t Label: 1\t Bag Size: 3720\n",
            "Epoch 16:\t Batch 799\t Avg Loss: 0.5750\t Label: 1\t Bag Size: 6535\n",
            "### (Train Summary) ###\n",
            "Epoch 16:\tTrain loss: 0.5765\t Train acc: 0.6930\t Train auc: 0.7688\n",
            "### (Val Summary) ###\n",
            "Epoch 16:\t  Val loss: 0.6682\t   Val acc: 0.6050\t   Val auc: 0.7313\n",
            "Early-stopping counter updating: 2/5 -> 3/5...\n",
            "\n",
            "Epoch 17:\t Batch 199\t Avg Loss: 0.6217\t Label: 1\t Bag Size: 1594\n",
            "Epoch 17:\t Batch 399\t Avg Loss: 0.6043\t Label: 1\t Bag Size: 5042\n",
            "Epoch 17:\t Batch 599\t Avg Loss: 0.5981\t Label: 1\t Bag Size: 4271\n",
            "Epoch 17:\t Batch 799\t Avg Loss: 0.5737\t Label: 0\t Bag Size: 3418\n",
            "### (Train Summary) ###\n",
            "Epoch 17:\tTrain loss: 0.5760\t Train acc: 0.7109\t Train auc: 0.7726\n",
            "### (Val Summary) ###\n",
            "Epoch 17:\t  Val loss: 0.6160\t   Val acc: 0.6798\t   Val auc: 0.7326\n",
            "Resetting early-stopping counter: 0.6198 -> 0.6160...\n",
            "\n",
            "Epoch 18:\t Batch 199\t Avg Loss: 0.5472\t Label: 0\t Bag Size: 2924\n",
            "Epoch 18:\t Batch 399\t Avg Loss: 0.5504\t Label: 0\t Bag Size: 5320\n",
            "Epoch 18:\t Batch 599\t Avg Loss: 0.5530\t Label: 0\t Bag Size: 2522\n",
            "Epoch 18:\t Batch 799\t Avg Loss: 0.5583\t Label: 0\t Bag Size: 446\n",
            "### (Train Summary) ###\n",
            "Epoch 18:\tTrain loss: 0.5615\t Train acc: 0.6861\t Train auc: 0.7838\n",
            "### (Val Summary) ###\n",
            "Epoch 18:\t  Val loss: 0.6216\t   Val acc: 0.6405\t   Val auc: 0.7351\n",
            "Early-stopping counter updating: 0/5 -> 1/5...\n",
            "\n",
            "Epoch 19:\t Batch 199\t Avg Loss: 0.5789\t Label: 1\t Bag Size: 1063\n",
            "Epoch 19:\t Batch 399\t Avg Loss: 0.5672\t Label: 1\t Bag Size: 3959\n",
            "Epoch 19:\t Batch 599\t Avg Loss: 0.5663\t Label: 0\t Bag Size: 1455\n",
            "Epoch 19:\t Batch 799\t Avg Loss: 0.5632\t Label: 0\t Bag Size: 4642\n",
            "### (Train Summary) ###\n",
            "Epoch 19:\tTrain loss: 0.5647\t Train acc: 0.7148\t Train auc: 0.7823\n",
            "### (Val Summary) ###\n",
            "Epoch 19:\t  Val loss: 0.6092\t   Val acc: 0.6607\t   Val auc: 0.7385\n",
            "Resetting early-stopping counter: 0.6160 -> 0.6092...\n",
            "\n",
            "### (Test Summary) ###\n",
            "Epoch 19:\t Test loss: 0.5563\t  Test acc: 0.6837\t  Test auc: 0.8401\n"
          ]
        }
      ],
      "source": [
        "# 设置随机种子（用于可重现性）\n",
        "torch.manual_seed(2023)\n",
        "\n",
        "# 获取用于训练-验证-测试拆分评估的数据加载器\n",
        "feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
        "loader_kwargs = {\n",
        "    'batch_size': 1,\n",
        "    'num_workers': 2,\n",
        "    'pin_memory': False\n",
        "}\n",
        "train_dataset, val_dataset, test_dataset = [MILDataset(feats_dirpath, csv_fpath, which_split=split) for split in ['train', 'val', 'test']]\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
        "\n",
        "# 获取模型、优化器和损失函数\n",
        "device = torch.device('cpu')\n",
        "model = AverageMIL(input_dim=320, hidden_dim=64).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 设置训练-验证循环和提前停止\n",
        "num_epochs, min_early_stopping, patience, counter = 20, 10, 5, 0\n",
        "lowest_val_loss, best_model = np.inf, None\n",
        "all_train_logs, all_val_logs = [], []\n",
        "for epoch in range(num_epochs):\n",
        "    # 训练模型\n",
        "    train_log = traineval_epoch(epoch, model, train_loader, optimizer=optimizer, split='train', device=device, verbose=2, print_every=200)\n",
        "    # 验证模型\n",
        "    val_log = traineval_epoch(epoch, model, val_loader, optimizer=None, split='val', device=device, verbose=1)\n",
        "    val_loss = val_log['val loss']\n",
        "    # 提前停止：如果验证损失在 <min_early_stopping> 轮后 <patience> 轮内不下降，则提前停止模型训练\n",
        "    if (epoch > min_early_stopping):\n",
        "        if (val_loss < lowest_val_loss):\n",
        "            print(f'重置提前停止计数器: {lowest_val_loss:.04f} -> {val_loss:.04f}...')\n",
        "            lowest_val_loss, counter, best_model = val_loss, 0, copy.deepcopy(model)\n",
        "        else:\n",
        "            print(f'提前停止计数器更新: {counter}/{patience} -> {counter+1}/{patience}...')\n",
        "            counter += 1\n",
        "\n",
        "    if counter >= patience: break\n",
        "    print()\n",
        "\n",
        "# 在测试拆分上报告最佳模型（最低验证损失）\n",
        "best_model = model if (best_model is None) else best_model\n",
        "test_log = traineval_epoch(epoch, best_model, test_loader, optimizer=None, split='test', device=device, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad43256",
      "metadata": {
        "id": "4ad43256"
      },
      "source": [
        "### 模型 2. 实现基于注意力的多实例学习 (ABMIL)\n",
        "\n",
        "在你对 `AverageMIL` 进行实验之后，你已经准备好为 LUAD vs. LUSC 亚型实现一个更复杂的模型了。形式上，令 $\\mathbf{H}=\\left\\{\\mathbf{h}_1, \\ldots, \\mathbf{h}_M\\right\\} \\in \\mathbb{R}^{M \\times D}$  为包含 $M$ 个补丁嵌入的一组袋子，其中每个嵌入的维度大小为 $D$。Ilse等人在2018年提出了以下基于注意力的MIL池化操作：\n",
        "\n",
        "$$\n",
        "\\mathbf{z} =\\sum_{i=1}^M a_i \\mathbf{h}_i, \\quad \\text{其中} \\enspace a_i=\\frac{\\exp \\left\\{\\mathbf{w}^{\\top}\\left(\\tanh \\left(\\mathbf{V h}_{i} ^ { \\top }\\right) \\odot \\operatorname{sigm}\\left(\\mathbf{U h}_i^{\\top}\\right)\\right)\\right\\}}{\\sum_{j=1}^M \\exp \\left\\{\\mathbf{w}^{\\top}\\left(\\tanh \\left(\\mathbf{V} \\mathbf{h}_j^{\\top}\\right) \\odot \\operatorname{sigm}\\left(\\mathbf{U h}_j^{\\top}\\right)\\right)\\right\\}}\n",
        "$$\n",
        "\n",
        "其中 $\\mathbf{w} \\in \\mathbb{R}^{L \\times 1}$, $\\mathbf{V} \\in \\mathbb{R}^{L \\times D}$, 和 $\\mathbf{U} \\in \\mathbb{R}^{L \\times D}$ 是可学习的神经网络参数（实现为全连接层），而 $\\mathbf{z} \\in \\mathbb{R}^{D}$ 则是 $\\mathbf{H}$ 中所有补丁嵌入的加权平均值。双曲正切 $\\tanh (\\cdot)$ 逐元素非线性和 sigmoid 非线性被用于适当的梯度流。\n",
        "\n",
        "通过 PyTorch，计算 $a_m$ 的数学表达式被实现为 `torch.nn` 模块 `AttentionTanhSigmoidGating`，我们将其作为 `ABMIL` 中的一个层来计算补丁嵌入的加权平均值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "09da5f05",
      "metadata": {
        "id": "09da5f05"
      },
      "outputs": [],
      "source": [
        "class AttentionTanhSigmoidGating(nn.Module):\n",
        "    def __init__(self, D=64, L=64, dropout=0.25):\n",
        "        r\"\"\"\n",
        "        带有双曲正切非线性和 Sigmoid 门控的全局注意力池化层（Ilse et al. 2018）。\n",
        "\n",
        "        Args:\n",
        "            D (int): 输入特征维度。\n",
        "            L (int): 隐藏层维度。符号从 Ilse et al 2018 中的 M 改为 L，因为 M 也用于描述 WSI 中的补丁嵌入数量。\n",
        "            dropout (float): Dropout 概率。\n",
        "\n",
        "        Returns:\n",
        "            A_norm (torch.Tensor): 归一化注意力分数的 [M x 1] 维张量（总和为 1）。\n",
        "        \"\"\"\n",
        "        super(AttentionTanhSigmoidGating, self).__init__()\n",
        "        self.tanhV = nn.Sequential(*[nn.Linear(D, L), nn.Tanh(), nn.Dropout(dropout)])\n",
        "        self.sigmU = nn.Sequential(*[nn.Linear(D, L), nn.Sigmoid(), nn.Dropout(dropout)])\n",
        "        self.w = nn.Linear(L, 1)\n",
        "\n",
        "    def forward(self, H):\n",
        "        A_raw = self.w(self.tanhV(H).mul(self.sigmU(H))) # 指数项\n",
        "        A_norm = F.softmax(A_raw, dim=0)                 # 应用 softmax 将权重归一化为 1\n",
        "        assert abs(A_norm.sum() - 1) < 1e-3              # 断言语句，用于检查 sum(A) 是否约等于 1\n",
        "        return A_norm\n",
        "\n",
        "\n",
        "class ABMIL(nn.Module):\n",
        "    def __init__(self, input_dim=320, hidden_dim=64, dropout=0.25, n_classes=2):\n",
        "        r\"\"\"\n",
        "        基于注意力的多实例学习（Ilse et al. 2018）。\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): 输入特征维度。\n",
        "            hidden_dim (int): 隐藏层维度。\n",
        "            dropout (float): Dropout 概率。\n",
        "            n_classes (int): 类别数量。\n",
        "        \"\"\"\n",
        "        super(ABMIL, self).__init__()\n",
        "        self.inst_level_fc = nn.Sequential(*[nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]) # 对每个嵌入应用“实例级”的全连接层\n",
        "        self.global_attn = AttentionTanhSigmoidGating(L=hidden_dim, D=hidden_dim)                              # 注意力函数\n",
        "        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)                                            # 袋子级别的分类器\n",
        "\n",
        "    def forward(self, X: torch.randn(100, 320)):\n",
        "        r\"\"\"\n",
        "        接受一个 [M x D] 维的补丁特征包（表示一个 WSI），并输出：1）用于分类的未归一化对数，2）未归一化的注意力分数。\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): [M x D] 维的补丁特征包（表示一个 WSI）\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): 用于分类任务的未归一化对数的 [1 x n_classes] 维张量。\n",
        "            A_norm (torch.Tensor): 注意力分数的 [M,]- 或 [M x 1] 维张量。\n",
        "        \"\"\"\n",
        "        H_inst = self.inst_level_fc(X)         # 1. 处理每个特征嵌入，使其大小为“hidden-dim”\n",
        "        A_norm = self.global_attn(H_inst)      # 2. 获取每个嵌入的归一化注意力分数（使 sum(A_norm) 约等于 1）\n",
        "        z = torch.sum(A_norm * H_inst, dim=0)  # 3. 对补丁进行全局注意力池化得到输出\n",
        "        logits = self.bag_level_classifier(z).unsqueeze(dim=0)   # 4. 获取用于分类任务的未归一化对数\n",
        "        try:\n",
        "            assert logits.shape == (1,2)\n",
        "        except:\n",
        "            print(f\"Logit tensor shape is not formatted correctly. Should output [1 x 2] shape, but got {logits.shape} shape\")\n",
        "        return logits, A_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "703caf8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "703caf8b",
        "outputId": "7e130c26-fb57-4cb9-e6a7-19ac828a74c1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a206111-ef0f-44b7-a45f-c5df042b0d8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_id</th>\n",
              "      <th>slide_id</th>\n",
              "      <th>tumor_type</th>\n",
              "      <th>OncoTreeSiteCode</th>\n",
              "      <th>main_cancer_type</th>\n",
              "      <th>sex</th>\n",
              "      <th>project_id</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>OncoTreeCode</th>\n",
              "      <th>OncoTreeCode_Binarized</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-73-4676</td>\n",
              "      <td>TCGA-73-4676-01Z-00-DX1.4d781bbc-a45e-4f9d-b6b...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-MP-A4T6</td>\n",
              "      <td>TCGA-MP-A4T6-01Z-00-DX1.085C4F5A-DB1B-434A-9D6...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-78-7167</td>\n",
              "      <td>TCGA-78-7167-01Z-00-DX1.f79e1a9b-a3eb-4c91-a1f...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-L9-A444</td>\n",
              "      <td>TCGA-L9-A444-01Z-00-DX1.88CF6F01-0C1F-4572-81E...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-55-8097</td>\n",
              "      <td>TCGA-55-8097-01Z-00-DX1.2f847b65-a5dc-41be-9dd...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>F</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TCGA-44-8119</td>\n",
              "      <td>TCGA-44-8119-01Z-00-DX1.1EBEBFA7-22DB-4365-9DF...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TCGA-49-AAR2</td>\n",
              "      <td>TCGA-49-AAR2-01Z-00-DX1.1F09F896-446E-4C55-8D0...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>TCGA-L9-A743</td>\n",
              "      <td>TCGA-L9-A743-01Z-00-DX1.27ED2955-E8B5-4A3C-ADA...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TCGA-99-8032</td>\n",
              "      <td>TCGA-99-8032-01Z-00-DX1.7380b78f-ea25-43e0-ac9...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TCGA-55-6972</td>\n",
              "      <td>TCGA-55-6972-01Z-00-DX1.0b441ad0-c30f-4f63-849...</td>\n",
              "      <td>Primary</td>\n",
              "      <td>LUNG</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>M</td>\n",
              "      <td>TCGA-LUAD</td>\n",
              "      <td>Lung Adenocarcinoma</td>\n",
              "      <td>LUAD</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a206111-ef0f-44b7-a45f-c5df042b0d8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a206111-ef0f-44b7-a45f-c5df042b0d8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a206111-ef0f-44b7-a45f-c5df042b0d8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        case_id                                           slide_id tumor_type  \\\n",
              "0  TCGA-73-4676  TCGA-73-4676-01Z-00-DX1.4d781bbc-a45e-4f9d-b6b...    Primary   \n",
              "1  TCGA-MP-A4T6  TCGA-MP-A4T6-01Z-00-DX1.085C4F5A-DB1B-434A-9D6...    Primary   \n",
              "2  TCGA-78-7167  TCGA-78-7167-01Z-00-DX1.f79e1a9b-a3eb-4c91-a1f...    Primary   \n",
              "3  TCGA-L9-A444  TCGA-L9-A444-01Z-00-DX1.88CF6F01-0C1F-4572-81E...    Primary   \n",
              "4  TCGA-55-8097  TCGA-55-8097-01Z-00-DX1.2f847b65-a5dc-41be-9dd...    Primary   \n",
              "5  TCGA-44-8119  TCGA-44-8119-01Z-00-DX1.1EBEBFA7-22DB-4365-9DF...    Primary   \n",
              "6  TCGA-49-AAR2  TCGA-49-AAR2-01Z-00-DX1.1F09F896-446E-4C55-8D0...    Primary   \n",
              "7  TCGA-L9-A743  TCGA-L9-A743-01Z-00-DX1.27ED2955-E8B5-4A3C-ADA...    Primary   \n",
              "8  TCGA-99-8032  TCGA-99-8032-01Z-00-DX1.7380b78f-ea25-43e0-ac9...    Primary   \n",
              "9  TCGA-55-6972  TCGA-55-6972-01Z-00-DX1.0b441ad0-c30f-4f63-849...    Primary   \n",
              "\n",
              "  OncoTreeSiteCode            main_cancer_type sex project_id  \\\n",
              "0             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "1             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "2             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "3             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "4             LUNG  Non-Small Cell Lung Cancer   F  TCGA-LUAD   \n",
              "5             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "6             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "7             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "8             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "9             LUNG  Non-Small Cell Lung Cancer   M  TCGA-LUAD   \n",
              "\n",
              "             Diagnosis OncoTreeCode  OncoTreeCode_Binarized  split  \n",
              "0  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "1  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "2  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "3  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "4  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "5  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "6  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "7  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "8  Lung Adenocarcinoma         LUAD                       0  train  \n",
              "9  Lung Adenocarcinoma         LUAD                       0  train  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:\t Batch 199\t Avg Loss: 0.7363\t Label: 0\t Bag Size: 507\n",
            "Epoch 0:\t Batch 399\t Avg Loss: 0.7275\t Label: 0\t Bag Size: 1163\n",
            "Epoch 0:\t Batch 599\t Avg Loss: 0.7064\t Label: 1\t Bag Size: 4417\n",
            "Epoch 0:\t Batch 799\t Avg Loss: 0.7082\t Label: 0\t Bag Size: 1598\n",
            "### (Train Summary) ###\n",
            "Epoch 0:\tTrain loss: 0.7074\t Train acc: 0.5111\t Train auc: 0.5243\n",
            "### (Val Summary) ###\n",
            "Epoch 0:\t  Val loss: 0.6894\t   Val acc: 0.4991\t   Val auc: 0.6386\n",
            "\n",
            "Epoch 1:\t Batch 199\t Avg Loss: 0.7016\t Label: 0\t Bag Size: 6939\n",
            "Epoch 1:\t Batch 399\t Avg Loss: 0.6918\t Label: 0\t Bag Size: 1229\n",
            "Epoch 1:\t Batch 599\t Avg Loss: 0.6928\t Label: 0\t Bag Size: 519\n",
            "Epoch 1:\t Batch 799\t Avg Loss: 0.6887\t Label: 1\t Bag Size: 4335\n",
            "### (Train Summary) ###\n",
            "Epoch 1:\tTrain loss: 0.6899\t Train acc: 0.5462\t Train auc: 0.5605\n",
            "### (Val Summary) ###\n",
            "Epoch 1:\t  Val loss: 0.6863\t   Val acc: 0.5000\t   Val auc: 0.6586\n",
            "\n",
            "Epoch 2:\t Batch 199\t Avg Loss: 0.6807\t Label: 1\t Bag Size: 7186\n",
            "Epoch 2:\t Batch 399\t Avg Loss: 0.6799\t Label: 1\t Bag Size: 2611\n",
            "Epoch 2:\t Batch 599\t Avg Loss: 0.6813\t Label: 0\t Bag Size: 708\n",
            "Epoch 2:\t Batch 799\t Avg Loss: 0.6780\t Label: 1\t Bag Size: 4235\n",
            "### (Train Summary) ###\n",
            "Epoch 2:\tTrain loss: 0.6780\t Train acc: 0.5782\t Train auc: 0.6022\n",
            "### (Val Summary) ###\n",
            "Epoch 2:\t  Val loss: 0.6763\t   Val acc: 0.5595\t   Val auc: 0.6739\n",
            "\n",
            "Epoch 3:\t Batch 199\t Avg Loss: 0.6632\t Label: 1\t Bag Size: 4657\n",
            "Epoch 3:\t Batch 399\t Avg Loss: 0.6718\t Label: 1\t Bag Size: 6929\n",
            "Epoch 3:\t Batch 599\t Avg Loss: 0.6729\t Label: 0\t Bag Size: 4582\n",
            "Epoch 3:\t Batch 799\t Avg Loss: 0.6697\t Label: 1\t Bag Size: 2143\n",
            "### (Train Summary) ###\n",
            "Epoch 3:\tTrain loss: 0.6716\t Train acc: 0.5818\t Train auc: 0.6167\n",
            "### (Val Summary) ###\n",
            "Epoch 3:\t  Val loss: 0.7256\t   Val acc: 0.5000\t   Val auc: 0.6705\n",
            "\n",
            "Epoch 4:\t Batch 199\t Avg Loss: 0.6751\t Label: 0\t Bag Size: 681\n",
            "Epoch 4:\t Batch 399\t Avg Loss: 0.6710\t Label: 0\t Bag Size: 4601\n",
            "Epoch 4:\t Batch 599\t Avg Loss: 0.6649\t Label: 0\t Bag Size: 3559\n",
            "Epoch 4:\t Batch 799\t Avg Loss: 0.6613\t Label: 0\t Bag Size: 287\n",
            "### (Train Summary) ###\n",
            "Epoch 4:\tTrain loss: 0.6596\t Train acc: 0.5896\t Train auc: 0.6441\n",
            "### (Val Summary) ###\n",
            "Epoch 4:\t  Val loss: 0.6605\t   Val acc: 0.6794\t   Val auc: 0.6913\n",
            "\n",
            "Epoch 5:\t Batch 199\t Avg Loss: 0.6653\t Label: 1\t Bag Size: 1096\n",
            "Epoch 5:\t Batch 399\t Avg Loss: 0.6557\t Label: 1\t Bag Size: 4743\n",
            "Epoch 5:\t Batch 599\t Avg Loss: 0.6457\t Label: 0\t Bag Size: 4152\n",
            "Epoch 5:\t Batch 799\t Avg Loss: 0.6487\t Label: 0\t Bag Size: 6785\n",
            "### (Train Summary) ###\n",
            "Epoch 5:\tTrain loss: 0.6471\t Train acc: 0.6375\t Train auc: 0.6780\n",
            "### (Val Summary) ###\n",
            "Epoch 5:\t  Val loss: 0.6603\t   Val acc: 0.5699\t   Val auc: 0.7007\n",
            "\n",
            "Epoch 6:\t Batch 199\t Avg Loss: 0.6459\t Label: 1\t Bag Size: 1111\n",
            "Epoch 6:\t Batch 399\t Avg Loss: 0.6323\t Label: 0\t Bag Size: 1265\n",
            "Epoch 6:\t Batch 599\t Avg Loss: 0.6366\t Label: 1\t Bag Size: 4928\n",
            "Epoch 6:\t Batch 799\t Avg Loss: 0.6297\t Label: 1\t Bag Size: 6929\n",
            "### (Train Summary) ###\n",
            "Epoch 6:\tTrain loss: 0.6280\t Train acc: 0.6475\t Train auc: 0.7111\n",
            "### (Val Summary) ###\n",
            "Epoch 6:\t  Val loss: 0.6431\t   Val acc: 0.6403\t   Val auc: 0.7062\n",
            "\n",
            "Epoch 7:\t Batch 199\t Avg Loss: 0.6292\t Label: 0\t Bag Size: 1555\n",
            "Epoch 7:\t Batch 399\t Avg Loss: 0.6124\t Label: 1\t Bag Size: 3609\n",
            "Epoch 7:\t Batch 599\t Avg Loss: 0.6162\t Label: 0\t Bag Size: 3900\n",
            "Epoch 7:\t Batch 799\t Avg Loss: 0.6058\t Label: 1\t Bag Size: 6125\n",
            "### (Train Summary) ###\n",
            "Epoch 7:\tTrain loss: 0.6057\t Train acc: 0.6645\t Train auc: 0.7375\n",
            "### (Val Summary) ###\n",
            "Epoch 7:\t  Val loss: 0.6388\t   Val acc: 0.7100\t   Val auc: 0.7177\n",
            "\n",
            "Epoch 8:\t Batch 199\t Avg Loss: 0.6160\t Label: 0\t Bag Size: 569\n",
            "Epoch 8:\t Batch 399\t Avg Loss: 0.6007\t Label: 1\t Bag Size: 3959\n",
            "Epoch 8:\t Batch 599\t Avg Loss: 0.5961\t Label: 0\t Bag Size: 3229\n",
            "Epoch 8:\t Batch 799\t Avg Loss: 0.5946\t Label: 0\t Bag Size: 4215\n",
            "### (Train Summary) ###\n",
            "Epoch 8:\tTrain loss: 0.5906\t Train acc: 0.6884\t Train auc: 0.7590\n",
            "### (Val Summary) ###\n",
            "Epoch 8:\t  Val loss: 0.6215\t   Val acc: 0.6607\t   Val auc: 0.7338\n",
            "\n",
            "Epoch 9:\t Batch 199\t Avg Loss: 0.5595\t Label: 0\t Bag Size: 1061\n",
            "Epoch 9:\t Batch 399\t Avg Loss: 0.5697\t Label: 0\t Bag Size: 1080\n",
            "Epoch 9:\t Batch 599\t Avg Loss: 0.5724\t Label: 1\t Bag Size: 1111\n",
            "Epoch 9:\t Batch 799\t Avg Loss: 0.5676\t Label: 1\t Bag Size: 2370\n",
            "### (Train Summary) ###\n",
            "Epoch 9:\tTrain loss: 0.5709\t Train acc: 0.7127\t Train auc: 0.7802\n",
            "### (Val Summary) ###\n",
            "Epoch 9:\t  Val loss: 0.6258\t   Val acc: 0.6996\t   Val auc: 0.7474\n",
            "\n",
            "Epoch 10:\t Batch 199\t Avg Loss: 0.5501\t Label: 0\t Bag Size: 657\n",
            "Epoch 10:\t Batch 399\t Avg Loss: 0.5604\t Label: 0\t Bag Size: 4582\n",
            "Epoch 10:\t Batch 599\t Avg Loss: 0.5507\t Label: 1\t Bag Size: 2964\n",
            "Epoch 10:\t Batch 799\t Avg Loss: 0.5445\t Label: 0\t Bag Size: 1877\n",
            "### (Train Summary) ###\n",
            "Epoch 10:\tTrain loss: 0.5479\t Train acc: 0.7466\t Train auc: 0.8090\n",
            "### (Val Summary) ###\n",
            "Epoch 10:\t  Val loss: 0.6297\t   Val acc: 0.6418\t   Val auc: 0.7721\n",
            "\n",
            "Epoch 11:\t Batch 199\t Avg Loss: 0.5339\t Label: 1\t Bag Size: 4775\n",
            "Epoch 11:\t Batch 399\t Avg Loss: 0.5165\t Label: 0\t Bag Size: 1163\n",
            "Epoch 11:\t Batch 599\t Avg Loss: 0.5147\t Label: 0\t Bag Size: 1045\n",
            "Epoch 11:\t Batch 799\t Avg Loss: 0.5211\t Label: 1\t Bag Size: 3985\n",
            "### (Train Summary) ###\n",
            "Epoch 11:\tTrain loss: 0.5204\t Train acc: 0.7333\t Train auc: 0.8233\n",
            "### (Val Summary) ###\n",
            "Epoch 11:\t  Val loss: 0.5948\t   Val acc: 0.7517\t   Val auc: 0.7874\n",
            "Resetting early-stopping counter: inf -> 0.5948...\n",
            "\n",
            "Epoch 12:\t Batch 199\t Avg Loss: 0.5097\t Label: 1\t Bag Size: 7475\n",
            "Epoch 12:\t Batch 399\t Avg Loss: 0.4951\t Label: 1\t Bag Size: 2272\n",
            "Epoch 12:\t Batch 599\t Avg Loss: 0.5076\t Label: 0\t Bag Size: 1681\n",
            "Epoch 12:\t Batch 799\t Avg Loss: 0.4966\t Label: 1\t Bag Size: 4230\n",
            "### (Train Summary) ###\n",
            "Epoch 12:\tTrain loss: 0.4967\t Train acc: 0.7786\t Train auc: 0.8461\n",
            "### (Val Summary) ###\n",
            "Epoch 12:\t  Val loss: 0.5608\t   Val acc: 0.7324\t   Val auc: 0.8014\n",
            "Resetting early-stopping counter: 0.5948 -> 0.5608...\n",
            "\n",
            "Epoch 13:\t Batch 199\t Avg Loss: 0.5048\t Label: 0\t Bag Size: 1846\n",
            "Epoch 13:\t Batch 399\t Avg Loss: 0.5039\t Label: 0\t Bag Size: 4170\n",
            "Epoch 13:\t Batch 599\t Avg Loss: 0.4992\t Label: 1\t Bag Size: 5204\n",
            "Epoch 13:\t Batch 799\t Avg Loss: 0.4776\t Label: 1\t Bag Size: 2459\n",
            "### (Train Summary) ###\n",
            "Epoch 13:\tTrain loss: 0.4785\t Train acc: 0.7714\t Train auc: 0.8557\n",
            "### (Val Summary) ###\n",
            "Epoch 13:\t  Val loss: 0.6134\t   Val acc: 0.6728\t   Val auc: 0.8134\n",
            "Early-stopping counter updating: 0/5 -> 1/5...\n",
            "\n",
            "Epoch 14:\t Batch 199\t Avg Loss: 0.4360\t Label: 1\t Bag Size: 1246\n",
            "Epoch 14:\t Batch 399\t Avg Loss: 0.4248\t Label: 1\t Bag Size: 511\n",
            "Epoch 14:\t Batch 599\t Avg Loss: 0.4275\t Label: 0\t Bag Size: 2878\n",
            "Epoch 14:\t Batch 799\t Avg Loss: 0.4469\t Label: 1\t Bag Size: 4986\n",
            "### (Train Summary) ###\n",
            "Epoch 14:\tTrain loss: 0.4499\t Train acc: 0.7972\t Train auc: 0.8744\n",
            "### (Val Summary) ###\n",
            "Epoch 14:\t  Val loss: 0.6795\t   Val acc: 0.6118\t   Val auc: 0.8355\n",
            "Early-stopping counter updating: 1/5 -> 2/5...\n",
            "\n",
            "Epoch 15:\t Batch 199\t Avg Loss: 0.4929\t Label: 0\t Bag Size: 8899\n",
            "Epoch 15:\t Batch 399\t Avg Loss: 0.4438\t Label: 1\t Bag Size: 2629\n",
            "Epoch 15:\t Batch 599\t Avg Loss: 0.4495\t Label: 1\t Bag Size: 5989\n",
            "Epoch 15:\t Batch 799\t Avg Loss: 0.4487\t Label: 1\t Bag Size: 3173\n",
            "### (Train Summary) ###\n",
            "Epoch 15:\tTrain loss: 0.4431\t Train acc: 0.8009\t Train auc: 0.8764\n",
            "### (Val Summary) ###\n",
            "Epoch 15:\t  Val loss: 0.7838\t   Val acc: 0.5508\t   Val auc: 0.8304\n",
            "Early-stopping counter updating: 2/5 -> 3/5...\n",
            "\n",
            "Epoch 16:\t Batch 199\t Avg Loss: 0.4658\t Label: 1\t Bag Size: 2192\n",
            "Epoch 16:\t Batch 399\t Avg Loss: 0.4532\t Label: 0\t Bag Size: 8059\n",
            "Epoch 16:\t Batch 599\t Avg Loss: 0.4323\t Label: 0\t Bag Size: 3019\n",
            "Epoch 16:\t Batch 799\t Avg Loss: 0.4167\t Label: 0\t Bag Size: 5014\n",
            "### (Train Summary) ###\n",
            "Epoch 16:\tTrain loss: 0.4073\t Train acc: 0.8277\t Train auc: 0.8994\n",
            "### (Val Summary) ###\n",
            "Epoch 16:\t  Val loss: 0.6598\t   Val acc: 0.6322\t   Val auc: 0.8410\n",
            "Early-stopping counter updating: 3/5 -> 4/5...\n",
            "\n",
            "Epoch 17:\t Batch 199\t Avg Loss: 0.4421\t Label: 0\t Bag Size: 2352\n",
            "Epoch 17:\t Batch 399\t Avg Loss: 0.4002\t Label: 1\t Bag Size: 502\n",
            "Epoch 17:\t Batch 599\t Avg Loss: 0.3995\t Label: 0\t Bag Size: 5177\n",
            "Epoch 17:\t Batch 799\t Avg Loss: 0.4163\t Label: 1\t Bag Size: 2739\n",
            "### (Train Summary) ###\n",
            "Epoch 17:\tTrain loss: 0.4133\t Train acc: 0.8065\t Train auc: 0.8932\n",
            "### (Val Summary) ###\n",
            "Epoch 17:\t  Val loss: 0.5700\t   Val acc: 0.6828\t   Val auc: 0.8520\n",
            "Early-stopping counter updating: 4/5 -> 5/5...\n",
            "### (Test Summary) ###\n",
            "Epoch 17:\t Test loss: 0.5071\t  Test acc: 0.7755\t  Test auc: 0.8917\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 设置随机种子（用于可重复性）\n",
        "torch.manual_seed(2023)\n",
        "\n",
        "# 获取用于训练-验证-测试分割评估的数据加载器\n",
        "if use_drive:\n",
        "  feats_dirpath = '/content/drive/My Drive/ai4healthsummerschool/feats_pt/'\n",
        "  csv_fpath = '/content/drive/My Drive/ai4healthsummerschool/tcga_lung_splits.csv'\n",
        "else:\n",
        "  feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
        "\n",
        "display(pd.read_csv(csv_fpath).head(10)) # 可视化数据\n",
        "loader_kwargs = {'batch_size': 1, 'num_workers': 2, 'pin_memory': False} # 由于可变大小的包大小，批处理大小设置为 1。难以整合。\n",
        "train_dataset, val_dataset, test_dataset = [MILDataset(feats_dirpath, csv_fpath, which_split=split) for split in ['train', 'val', 'test']]\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
        "\n",
        "# 获取模型、优化器和损失函数\n",
        "device = torch.device('cpu')\n",
        "model = ABMIL(input_dim=320, hidden_dim=64).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 设置训练-验证循环和提前停止\n",
        "num_epochs, min_early_stopping, patience, counter = 20, 10, 5, 0\n",
        "lowest_val_loss, best_model = np.inf, None\n",
        "all_train_logs, all_val_logs = [], [] # TODO: 每个时期对 train_log / val_log 进行一些操作以帮助可视化性能曲线？\n",
        "for epoch in range(num_epochs):\n",
        "    train_log = traineval_epoch(epoch, model, train_loader, optimizer=optimizer, split='train', device=device, verbose=2, print_every=200)\n",
        "    val_log = traineval_epoch(epoch, model, val_loader, optimizer=None, split='val', device=device, verbose=1)\n",
        "    val_loss = val_log['val loss']\n",
        "\n",
        "    # 提前停止：如果验证损失在最小提前停止之后的 <patience> 个时期内没有下降，提前停止模型训练\n",
        "    if (epoch > min_early_stopping):\n",
        "        if (val_loss < lowest_val_loss):\n",
        "            print(f'Resetting early-stopping counter: {lowest_val_loss:.04f} -> {val_loss:.04f}...')\n",
        "            lowest_val_loss, counter, best_model = val_loss, 0, copy.deepcopy(model)\n",
        "        else:\n",
        "            print(f'Early-stopping counter updating: {counter}/{patience} -> {counter+1}/{patience}...')\n",
        "            counter += 1\n",
        "\n",
        "    if counter >= patience: break\n",
        "    print()\n",
        "\n",
        "# 在测试分割上报告最佳模型（最低验证损失）\n",
        "best_model = model if (best_model is None) else best_model\n",
        "test_log = traineval_epoch(epoch, best_model, test_loader, optimizer=None, split='test', device=device, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d450527",
      "metadata": {
        "id": "7d450527"
      },
      "source": [
        "### 讨论. 比较和对比 AverageMIL 和 ABMIL\n",
        "\n",
        "比较和对比 `AverageMIL` 和 `ABMIL` 在**验证**和**测试**性能上的表现。具体而言：\n",
        "\n",
        "2. 哪个模型在**测试集**上的整体AUC和平衡准确度表现更好？每个模型更容易将哪个类别（LUAD还是LUSC）误分类？\n",
        "3. 在[http://clam.mahmoodlab.org](http://clam.mahmoodlab.org)提供了针对LUAD和LUSC亚型的高注意力热图可视化，通过CLAM（类似于`ABMIL`），以及每张幻灯片的置信度分数。作为一名临床病理学家，观察这些可视化结果，您对让AI算法辅助您的医学诊断有哪些见解或担忧？\n",
        "4. 这个问题集中的实验设置仅限于对来自TCGA的数据进行评估。列出Lu等人2021年（或其他相关的生物医学成像× AI研究）中使用的三种技术，可以用于评估1）数据效率，2）泛化性能和3）`ABMIL`的基于注意力的可解释性的一致性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f56d08cc",
      "metadata": {
        "id": "f56d08cc"
      },
      "outputs": [],
      "source": [
        "# save best_model for next session\n",
        "if use_drive:\n",
        "  torch.save(best_model.state_dict(), '/content/drive/My Drive/ai4healthsummerschool/abmil.ckpt')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ab8319496eb09321cf5597fd6abbcdcc0a1b57046a5c3edfd9bd1380f7f3d177"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
