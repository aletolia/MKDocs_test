## Abstract 

基础模型目前为深度学习领域大多数令人兴奋的应用提供了动力，这些模型几乎都基于 Transformer 架构及其核心注意力模块。为了解决 Transformer 在长序列上的计算效率低下问题，人们开发了许多亚二次时间架构，如线性注意力、门控卷积和递归模型以及结构化状态空间模型（SSM），但它们在语言等重要模态上的表现不如注意力。我们发现，此类模型的一个关键弱点是无法进行基于内容的推理，因此做出了几项改进。首先，只需让 SSM 参数成为输入的函数，就能解决它们在离散模态方面的弱点，使模型能够根据当前标记，有选择地沿序列长度维度传播或遗忘信息。其次，尽管这种变化阻碍了高效卷积的使用，我们还是设计了一种硬件感知的并行递归模式算法。我们将这些选择性 SSM 集成到一个简化的端到端神经网络架构中，该架构没有注意力，甚至没有 MLP 块（Mamba）。Mamba 具有快速推理（吞吐量比 Transformers 高 5 倍）和序列长度线性伸缩的特点，其性能在实际数据中可提高到百万长度序列。作为通用序列模型的骨干，Mamba 在语言、音频和基因组学等多种模式中都达到了最先进的性能。在语言建模方面，无论是预训练还是下游评估，我们的 Mamba-3B 模型都优于同等规模的 Transformers，并能与两倍于其规模的 Transformers 相媲美。

## 1 Introduction 

基础模型（FM），即在海量数据上进行预训练，然后针对下游任务进行调整的大型模型，已成为现代机器学习的有效范式。这些基础模型的骨干通常是序列模型，对来自语言、图像、语音、音频、时间序列和基因组学等广泛领域的任意输入序列进行操作（Brown 等，2020 年；Dosovitskiy 等，2020 年；Ismail Fawaz 等，2019 年；Oord 等，2016 年；Poli 等，2023 年；Sutskever、Vinyals 和 Quoc V Le，2014 年）。虽然这一概念与模型架构的特定选择无关，但现代调频主要基于一种单一类型的序列模型：Transformer（Vaswani 等人，2017 年）及其核心注意力层（Bahdanau、Cho 和 Bengio，2015 年）。自我注意力的功效归功于其在上下文窗口内密集路由信息的能力，使其能够对复杂数据进行建模。然而，这一特性也带来了一些根本性的缺陷：无法对有限窗口外的任何内容进行建模，以及窗口长度的二次缩放。为了克服这些缺点，人们对注意力的更有效变体进行了大量研究（Tay、Dehghani、Bahri 等，2022 年），但这些研究往往以牺牲注意力的有效特性为代价。到目前为止，这些变体还没有任何一种在跨领域的规模上显示出经验上的有效性。

最近，结构化状态空间序列模型（SSMs）（Gu、Goel 和 Ré 2022 年；Gu、Johnson、Goel 等 2021 年）已成为一类很有前途的序列建模架构。这些模型可以解释为递归神经网络（RNN）和卷积神经网络（CNN）的结合，其灵感来自经典的状态空间模型（卡尔曼，1960 年）。这类模型可以以递归或卷积的方式高效计算，序列长度呈线性或近似线性扩展。此外，在某些数据模式中，它们还具有建模长程依赖性的原理机制（Gu、Dao 等，2020 年），并在长程竞技场等基准测试中占据主导地位（Tay、Dehghani、Abnar 等，2021 年）。许多 SSM（Gu、Goel 和 Ré 2022 年；Gu、Gupta 等 2022 年；Gupta、Gu 和 Berant 2022 年；Y. Li 等 2023 年；Ma 等 2023 年；Orvieto 等 2023 年；Smith、Warrington、Abnar 等 2023 年）在某些数据模式中占据主导地位。2023；Smith、Warrington 和 Linderman 2023）在音频和视觉等涉及连续信号数据的领域取得了成功（Goel 等人，2022；Nguyen、Goel 等人，2022；Saon、Gupta 和 Cui 2023）。然而，它们在离散和信息密集型数据（如文本）建模方面却不太有效。

我们提出了一类新的选择性状态空间模型，该模型在多个轴上改进了之前的工作，以实现变形模型的建模能力，同时按序列长度线性扩展。

选择机制。首先，我们发现了先前模型的一个关键局限：以依赖输入的方式有效选择数据的能力（即关注或忽略特定输入）。基于选择性复制和感应头等重要合成任务的直觉，我们设计了一种简单的选择机制，根据输入对 SSM 参数进行参数化。这样，模型就能过滤掉无关信息，并无限期地记住相关信息。硬件感知算法。这一简单的改变对模型的计算提出了技术挑战；事实上，所有先前的 SSM 模型都必须是时间和输入不变的，这样才能提高计算效率。我们采用了一种硬件感知算法来克服这一难题，该算法通过扫描而不是卷积来计算模型，但不会将扩展状态具体化，以避免在 GPU 存储器层次结构的不同级别之间进行 IO 访问。由此产生的实现方法无论是在理论上（与所有基于卷积的 SSM 的伪线性相比，随序列长度线性扩展）还是在现代硬件上（在 A100 GPU 上可快达 3 倍）都比以前的方法更快。架构。我们将先前的 SSM 架构设计（Dao、Fu、Saab 等人，2023 年）与 Transformers 的 MLP 模块合并为一个模块，从而简化了先前的深度序列模型架构，形成了一种包含选择性状态空间的简单而同质的架构设计（Mamba）。选择性 SSM 以及 Mamba 架构是完全递归模型，其关键特性使其适合作为在序列上运行的通用基础模型的骨干。(i) 高质量：选择性为语言和基因组学等密集模式带来了强大的性能。(ii) 快速训练和推理：在训练过程中，计算量和内存与序列长度成线性关系，而在推理过程中，由于不需要缓存以前的元素，因此自回归展开模型每一步只需要恒定的时间。(iii) 上下文长：质量和效率共同提高了实际数据的性能，序列长度可达 100 万。

我们通过实证验证了曼巴作为通用序列 FM 骨干的潜力，无论是在预训练质量还是领域特定任务性能上，在多种类型的模态和设置下都表现出色：

• 合成数据。在重要的合成任务上，比如复制和归纳头部，曼巴不仅轻松解决了它们，而且可以无限长地外推解决方案（>1M 个标记）。

• 音频和基因组学。曼巴在对音频波形和 DNA 序列建模方面超越了之前的最先进模型，如 SaShiMi、Hyena 和 Transformers，无论是在预训练质量还是下游指标（例如，在具有挑战性的语音生成数据集上将 FID 减少了一半以上）。在这两种情况下，它的性能随着上下文长度增加而提高，直到百万长度的序列。

• 语言建模。曼巴是第一个真正实现了 Transformer 质量性能的线性时间序列模型，无论是在预训练困惑度还是下游评估中。通过扩展到 10 亿参数的缩放规律，我们展示了曼巴超过了大量基线的性能，包括基于 LLaMa 的非常强大的现代 Transformer 训练配方（Touvron 等人，2023 年）。我们的曼巴语言模型与相似大小的 Transformers 相比，有 5 倍的生成吞吐量，而曼巴 -3B 的质量与两倍大小的 Transformers 相匹配（例如，在常识推理方面，与 Pythia-3B 相比平均高出 4 个点，甚至超过 Pythia-7B）。

模型代码和预训练检查点在 https://github.com/state-spaces/mamba 上开源。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202403291929144.png)

图 1：（概述）结构化的 SSMs 通过更高维度的潜在状态 h（例如 N = 4）独立地将输入 x 的每个通道（例如 D = 5）映射到输出 y。先前的 SSMs 通过巧妙的替代计算路径避免了实现这个大的有效状态（DN，批次大小 B 和序列长度 L）要求时间不变性：参数（，A，B，C）在时间上保持恒定。我们的选择机制添加了与输入相关的动态性，这也需要一个精心设计的硬件感知算法，只在 GPU 内存层次结构的更高效级别中实现扩展状态。

## 2 状态空间模型

结构化状态空间序列模型（S4）是一类最近的深度学习序列模型，与 RNNs、CNNs 和经典状态空间模型广泛相关。它们受到特定连续系统（1）的启发，该系统通过隐式潜在状态 $h(t) \in \mathbb{R}^N$ 将一个 1 维函数或序列 $x(t) \in \mathbb{R}$ 映射到 $y(t) \in \mathbb{R}$。

具体来说，S4 模型用四个参数 $(\Delta, A, B, C)$ 来定义，这四个参数在两个阶段中定义了序列到序列的转换。
$$
\begin{aligned}
h^{\prime}(t) & =A h(t)+B x(t) \\
y(t) & =C h(t)
\end{aligned}
$$
$$
\begin{aligned}
h_t & =\bar{A} h_{t-1}+\bar{B} x_t \\
y_t & =C h_t
\end{aligned}
$$
$$
\begin{aligned}
\overline{\boldsymbol{K}} & =\left(C \overline{\boldsymbol{B}}, C \overline{\boldsymbol{A B}}, \ldots, C \overline{\boldsymbol{A}}^k \overline{\boldsymbol{B}}, \ldots\right) \\
y & =x * \overline{\boldsymbol{K}}
\end{aligned}
$$

**离散化**。第一阶段通过固定的公式将“连续参数”$(\Delta, A, B)$ 转换为“离散参数”$(\bar{A}, \bar{B})$，其中对 $(f_A, f_B)$ 成对称呼为离散化规则。可以使用各种规则，例如方程（4）中定义的零阶保持（$\mathrm{ZOH}$）。
$$
\bar{A}=\exp (\Delta A) \quad \bar{B}=(\Delta A)^{-1}(\exp (\Delta A)-I) \cdot \Delta B
$$

离散化与连续时间系统有着深刻的联系，这些联系可以赋予它们额外的属性，如分辨率不变性（Nguyen, Goel 等人，2022 年）和自动确保模型被适当地归一化（Gu, Johnson, Timalsina 等人，2023 年；Orvieto 等人，2023 年）。它还与 RNN 的门控机制有关（Gu, Gulcehre,等人，2020 年；Tallec 和 Ollivier，2018 年），我们将在第 3.5 节中重新讨论这一点。然而，从机械的角度来看，离散化可以简单地视为 S4 正向传播计算图的第一步。替代的 SSM 的变体可以绕过离散化步骤，直接对 $(\bar{A}, \bar{B})$ 进行参数化（Zhang 等人，2023 年），这样可能更容易理解。

**计算**。在参数从 $(\Delta, A, B, C) \mapsto(\bar{A}, \bar{B}, C)$ 进行转换后，模型可以通过两种方式进行计算，即作为线性递推（2）或全局卷积（3）。通常情况下，模型使用卷积模式（3）进行高效可并行化的训练（在整个输入序列之前就可见），并且在有效的自回归推理（一次只看到一个时间步的输入）中切换到递归模式（2）。

**线性时间不变性（LTI）**。方程（1）到（3）的一个重要属性是模型的动态在时间上是恒定的。换句话说，$(\Delta, A, B, C)$，因此 $(\bar{A}, \bar{B})$ 也是固定的，对于所有时间步而言都是不变的。这个属性被称为线性时间不变性（LTI），它与递归和卷积有着深刻的联系。简而言之，我们认为 LTI SSMs 等价于任何线性递归（2a）或卷积（3b），并且使用 LTI 作为这些模型类别的总称。

到目前为止，所有的结构化 SSMs 都是 LTI（例如计算为卷积），这是由于基本的效率约束，如第 3.3 节所讨论的。然而，这项工作的一个核心见解是 LTI 模型在建模某些类型的数据时具有根本性的局限性，我们的技术贡献包括消除 LTI 约束并克服效率瓶颈。

**结构和维度**。最后，我们注意到结构化的 SSMs 之所以被如此命名，是因为要有效地计算它们也需要对 $A$ 矩阵施加结构。最流行的结构形式是对角线结构（Gu, Gupta 等人，2022 年；Gupta, Gu 和 Berant，2022 年；Smith, Warrington 和 Linderman，2023 年），我们也使用这种结构。
在这种情况下，$A \in \mathbb{R}^{N \times N}, B \in \mathbb{R}^{N \times 1}, C \in \mathbb{R}^{1 \times N}$ 矩阵都可以由 $N$ 个数字表示。对于批次大小为 $B$ 且长度为 $L$，具有 $D$ 通道的输入序列 $x$，SSM 独立地应用于每个通道。请注意，在这种情况下，每个输入的总隐藏状态的维度为 $D N$，并且在整个序列长度上进行计算需要 $O(B L D N)$ 的时间和内存；这是第 3.3 节中解决的基本效率瓶颈的根源。

通用状态空间模型。我们注意到，术语状态空间模型具有非常广泛的含义，简单表示任何具有潜在状态的循环过程的概念。它已被用来指代不同学科中的许多不同概念，包括马尔可夫决策过程（MDP）（强化学习（Hafner 等人，2020 年）），动态因果建模（DCM）（计算神经科学（Friston，Harrison 和 Penny，2003 年）），卡尔曼滤波器（控制（Kalman，1960 年）），隐马尔可夫模型（HMM）和线性动态系统（LDS）（机器学习），以及大规模的循环（有时是卷积）模型（深度学习）。

在整篇论文中，我们使用术语“SSM”专指结构化 SSM 或 S4 模型类（Gu，Goel 和 Ré，2022 年；Gu，Gupta 等人，2022 年；Gupta，Gu 和 Berant，2022 年；Hasani 等人，2023 年；Ma 等人，2023 年；Smith，Warrington 和 Linderman，2023 年），并且这些术语可互换使用。为方便起见，我们还可以包括这些模型的衍生物，例如那些专注于线性递归或全局卷积观点的模型（Y. Li 等人，2023 年；Orvieto 等人，2023 年；Poli 等人，2023 年），并在必要时澄清细微差别。

**SSM 架构**。SSMs 是独立的序列转换，可以纳入端到端的神经网络架构中。（我们有时也将 SSM 架构称为 SSNNs，就像 CNNs 是线性卷积层一样。）我们讨论一些最著名的 SSM 架构，其中许多也将成为我们的主要基线。

- 线性注意力（Katharopoulos 等人，2020 年）是自注意力的一种近似，涉及一种可以视为退化线性 SSM 的循环。
- H3（Dao，Fu，Saab 等人，2023 年）将这种循环推广为使用 S4 的结构；可以将其视为一种在两个门控连接之间夹带 SSM 的架构（图 3）。H3 还在主要的 SSM 层之前插入了一个标准的局部卷积，他们将其框架化为一个位移 -SSM。
- Hyena（Poli 等人，2023 年）使用与 H3 相同的架构，但将 S4 层替换为 MLP 参数化的全局卷积（Romero 等人，2021 年）。
- RetNet（Y. Sun 等人，2023 年）在架构中添加了额外的门，并使用了更简单的 SSM，允许另一种可并行化的计算路径，使用多头注意力（MHA）的变体代替卷积。
- RWKV（B. Peng 等人，2023 年）是一种基于另一种线性注意力近似（无注意力 Transformer（S. Zhai 等人，2021 年））的最新 RNN，专为语言建模设计。其主要的“WKV”机制涉及 LTI 循环，并且可以视为两个 SSM 的比率。

其他与 SSM 密切相关的 SSMs 和架构在扩展的相关工作中进一步讨论（附录 B）。我们特别强调 S5（Smith，Warrington 和 Linderman，2023 年），QRNN（Bradbury 等人，2016 年）和 SRU（Lei 等人，2017 年），我们认为这些是与我们的核心选择性 SSM 最相关的方法。

## 3 选择性状态空间模型

我们使用合成任务（第 3.1 节）的直觉来推动我们的选择机制，然后解释如何将这种机制纳入状态空间模型（第 3.2 节）中。由此产生的时间变化的 SSMs 不能使用卷积，这提出了如何有效计算它们的技术挑战。我们通过一种硬件感知算法来克服这一挑战，该算法利用了现代硬件上的内存层次结构（第 3.3 节）。然后，我们描述了一个简单的 SSM 架构，没有注意力甚至 MLP 块（第 3.4 节）。最后，我们讨论了选择机制的一些附加属性（第 3.5 节）。

### 3.1 动机：选择作为压缩手段

我们认为序列建模的一个基本问题是将上下文压缩成一个较小的状态。事实上，我们可以从这个角度看待流行的序列模型的权衡。例如，注意力既有效又低效，因为它明确地不对上下文进行任何压缩。这可以从自回归推理需要显式存储整个上下文（即 KV 缓存）这一事实中看出，这直接导致了 Transformer 的缓慢线性时间推理和二次时间训练。另一方面，递归模型是高效的，因为它们具有有限的状态，意味着恒定时间的推理和线性时间的训练。然而，它们的有效性受限于该状态如何压缩上下文。
为了理解这个原理，我们关注两个合成任务的运行示例（图 2）。

- 选择性复制任务修改了流行的复制任务（Arjovsky，Shah 和 Bengio，2016 年），通过改变要记忆的令牌的位置。它需要内容感知推理，以便能够记忆相关令牌（着色）并过滤掉无关的令牌（白色）。
- 归纳头部任务是一个众所周知的机制，假设可以解释 LLMs（Olsson 等人，2022 年）大多数在上下文学习能力。它需要上下文感知推理，以知道何时在适当的上下文中产生正确的输出（黑色）。

这些任务揭示了 LTI 模型的失效模式。从递归视角来看，它们的恒定动态（例如（2）中的 $(\bar{A}, \bar{B})$ 转换）不能让它们从上下文中选择正确的信息，或者以输入相关的方式影响沿着序列传递的隐藏状态。从卷积视角来看，众所周知，全局卷积可以解决普通的复制任务（Romero 等人，2021 年），因为它只需要时间感知，但是它们在选择性复制任务上存在困难，因为缺乏内容感知（图 2）。更具体地说，输入到输出之间的间隔是变化的，并且不能通过静态卷积核来建模。

总之，序列模型的效率与有效性之间的权衡特征在于它们如何压缩其状态：高效的模型必须具有较小的状态，而有效的模型必须具有包含来自上下文的所有必要信息的状态。反过来，我们提出建立序列模型的一个基本原则是选择性：或者是对上下文进行感知的能力，以便集中于或过滤掉输入到顺序状态中。特别是，选择机制控制信息沿着序列维度传播或交互的方式（有关更多讨论，请参见第 3.5 节）。

### 3.2 使用选择改进 SSMs

将选择机制纳入模型的一种方法是让影响沿着序列交互的参数（例如 RNN 的递归动态或 CNN 的卷积核）取决于输入。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202403292051210.png)

图 2：（左）复制任务的标准版本涉及输入和输出元素之间的恒定间距，并且可以轻松通过时间不变模型（如线性递推和全局卷积）解决。（右上）选择性复制任务在输入之间具有随机间距，并且需要可以根据其内容选择性记忆或忽略输入的时间变化模型。（右下）归纳头任务是一种关联召回的示例，它需要根据上下文检索答案，这是 LLMs 的关键能力。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202403292052212.png)

> 输入 `x` 参数为 `(B→Batch size,L→length,D→Delta(time space))` 即输入的几个维度
>
> 根据 HiPPO matrix 方法，参数矩阵 A,B,C 均和输入被使用多项式逼近的阶数 $N$ 和时间间隔 $\Delta$ 有关，其中矩阵 A 会被重构为 $N\times N$ 的形式
>
> 接着根据 ZOH 矩阵 A 和 B 会根据时间间隔被离散化，然后通过一种类似卷积的方式输出 y （添加了残差连接 D）
>
> 在 S6 也就是 Mamba 中，参数矩阵 A 的形式仍然没有发生变化，但是参数矩阵 B 和 C 的输入形式则出现了变化，现在参数化矩阵 B,C 时，需要考虑长度（也就是参数 `Length` ）同时在离散化时的时间间隔也是一个基于参数 `L` 的时变参数，这使得模型成为了一个时变系统，能够做到类似注意力的选择性分析

算法 1 和算法 2 展示了我们使用的主要选择机制。主要区别仅在于将几个参数 $\Delta, B, C$ 变为输入的函数，以及与整个张量形状相关的变化。特别要强调的是，这些参数现在具有长度维度 $L$​，这意味着模型已从时间不变变为时间变化。（请注意，形状标注在第 2 节中描述。）这失去了与卷积（3）的等价性，对其效率产生了影响，下面进行讨论。

我们特别选择了 $s_B(x)=\operatorname{Linear}_N(x), s_C(x)=\operatorname{Linear}_N(x), s_{\Delta}(x)=\operatorname{Broadcast}_D\left(\operatorname{Linear}_1(x)\right)$，以及 $\tau_{\Delta}=$ softplus，其中 Linear ${ }_d$ 是一个参数化的投影到维度 $d$ 的函数。选择 $s_{\Delta}$ 和 $\tau_{\Delta}$ 是因为与 RNN 门控机制的连接，详见第 3.5 节。

### 3.3 选择性 SSM 的高效实现

硬件友好的架构，如卷积（Krizhevsky，Sutskever 和 Hinton，2012 年）和 Transformer（Vaswani 等人，2017 年）具有广泛的应用。在这里，我们的目标是使选择性 SSM 在现代硬件（GPU）上也能高效运行。选择机制非常自然，较早的工作尝试将特殊情况的选择纳入其中，例如在循环 SSM 中让 $\Delta$ 随时间变化（Gu，Dao 等人，2020 年）。然而，正如之前提到的，SSM 的一个核心局限性是它们的计算效率，这就是为什么 S4 和所有派生物都使用 LTI（非选择性）模型的核心原因，最常见的形式是全局卷积。

#### 3.3.1 先前模型的动机

我们首先重温这一动机,并概述我们克服先前方法局限性的方法。

- 从高层次来看,循环模型如 SSM 总是在表现力和速度之间权衡: 如第 3.1 节所讨论的,隐藏状态维数越大的模型应该更有效,但速度更慢。因此,我们希望在不付出速度和内存代价的情况下最大化隐藏状态维数。

- 请注意,循环模式比卷积模式更灵活,因为后者 (3) 是从扩展前者 (2) 而来的 (Gu、Goel 和 Ré 2022;Gu、Johnson、Goel 等人 2021)。然而,这需要计算并实体化形状为 (B, L, D, N) 的潜在状态 h,比输入 x 和输出 y 的形状 (B, L, D) 大了一个 N 的因子 (N 是 SSM 状态维数)。因此引入了更高效的卷积模式,它可以绕过状态计算,实体化仅形状为 (B, L, D) 的卷积核 (3a)。

- 先前的 LTI SSM 利用双重循环 - 卷积形式,将有效状态维数增加了 N(约 10-100) 倍,比传统 RNN 大得多,而不付出效率代价。

#### 3.3.2 选择性扫描: 硬件感知状态扩展概述  

选择机制旨在克服 LTI 模型的局限性; 因此,我们需要重新审视 SSM 的计算问题。我们采用三种经典技术来解决这个问题: 内核融合、并行扫描和重新计算。我们有两个主要观察结果:

- 朴素的循环计算使用 $O(BLDN)$ 次 FLOP,而卷积计算使用 $O(BLDLOG(L))$ 次 FLOP,前者的常数因子更低。因此对于长序列和不太大的状态维数 N,循环模式实际上可以使用更少的 FLOP。  

- 两个挑战是循环的顺序性质,以及大的内存使用量。为解决后者,就像卷积模式一样,我们可以尝试不实际实体化完整的状态 h。

主要思想是利用现代加速器 (GPU) 的属性,仅在更高效的内存层次中实体化状态 h。特别是,除了矩阵乘法之外,大多数操作都受制于内存带宽 (Dao、Fu、Ermon 等人 2022;Ivanov 等人 2021;Williams、Waterman 和 Patterson 2009)。这包括我们的扫描操作,我们使用内核融合来减少内存 IO 的数量,从而比标准实现快得多。

具体来说,我们不是在 GPU HBM(高带宽内存) 中准备大小为 (B, L, D, N) 的扫描输入 (A¯,B¯),而是直接从缓慢的 HBM 加载 SSM 参数 (Δ,A,B,C) 到快速 SRAM,在 SRAM 中进行离散化和循环,然后将最终大小为 (B, L, D) 的输出写回 HBM。  

为避免顺序循环,我们观察到尽管不是线性的,它仍然可以使用工作高效的并行扫描算法并行化 (Blelloch 1990;Martin 和 Cundy 2018;Smith、Warrington 和 Linderman 2023)。

最后,我们还必须避免保存中间状态,这对反向传播是必需的。我们小心地应用经典的重新计算技术来降低内存需求: 中间状态不保存,而是在反向传播时,当输入从 HBM 加载到 SRAM 时重新计算。因此,融合的选择性扫描层的内存需求与经过优化的具有 FlashAttention 的 transformer 实现相同。

关于融合内核和重新计算的细节,请参阅附录 D。完整的选择 SSM 层和算法如图 1 所示。

### 3.4 简化的 SSM 架构

与结构化 SSM 一样,选择性 SSM 是独立的序列转换,可灵活地并入神经网络中。H3 架构是最著名的 SSM 架构 (第 2 节) 的基础,通常由受线性注意力启发的块与多层感知器 (MLP) 块交织而成。我们通过将这两个组件合并为一个来简化该架构,并将其同质堆叠 (图 3)。这受到门控注意力单元 (GAU)(Hua et al. 2022) 的启发,后者对注意力做了类似的事情。

该架构涉及通过可控制的扩展因子 $E$ 来扩展模型维数 $D$。对于每个块,大部分参数 $\left(3 E D^2\right)$ 都在线性投影中 $\left(2 E D^2\right.$ 用于输入投影,$E D^2$ 用于输出投影),而内部 SSM 的贡献较少。SSM 参数的数量 (用于 $\Delta$、$B$、$C$ 以及矩阵 $A$ 的投影) 则相比之下要小得多。我们重复这个块,并交织标准归一化和残差连接,形成 Mamba 架构。在我们的实验中,我们总是固定 $E=2$,并使用该块的两个堆栈,以匹配 Transformer 的交织多头注意力 (MHA) 和 MLP 块的 $12 D^2$ 参数。我们使用 SiLU/Swish 激活函数 (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017),动机是使门控 MLP 成为流行的 "SwiGLU" 变体 (Chowdhery et al. 2023; Shazeer 2020; Touvron et al. 2023)。最后,我们还可选择使用归一化层 (我们选择 LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),这得到了 RetNet 在类似位置使用归一化层的启发 (Y. Sun et al. 2023)。

![image.png](https://raw.githubusercontent.com/aletolia/Pictures/main/202403292058807.png)

图 3:(架构) 我们简化的块设计将 H3 块 (是大多数 SSM 架构的基础) 与现代神经网络中无处不在的 MLP 块结合。我们没有交替这两个块,而是简单地同质重复 Mamba 块。与 H3 块相比,Mamba 用激活函数替换了第一个乘法门控。与 MLP 块相比,Mamba 在主分支中添加了一个 SSM。对于激活函数φ,我们使用 SiLU/Swish 激活函数 (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017)。

### 3.5 选择机制的性质

选择机制是一个更广泛的概念,可以通过不同的方式应用,比如应用于更传统的 RNN 或 CNN、应用于不同的参数 (如算法 2 中的 A) 或使用不同的变换 s(x)。

#### 3.5.1 与门控机制的联系

我们强调最重要的联系:RNN 的经典门控机制是我们为 SSM 的选择机制的一个实例。我们注意到,RNN 门控与连续时间系统的离散化之间的联系是众所周知的 (Funahashi 和 Nakamura 1993;Tallec 和 Ollivier 2018)。事实上,定理 1 是对 Gu、Johnson、Goel 等人 (2021,引理 3.1) 的改进,推广到 ZOH 离散化和输入依赖的门控 (证明见附录 C)。更广泛地说,SSM 中的 Δ 可以被看作是 RNN 门控机制的一种概括角色。与之前的工作一致,我们采纳这样的观点: SSM 的离散化是启发式门控机制的原则基础。

定理 1.当 $N=1,A=-1,B=1,sΔ=Linear(x),τΔ=softplus$ 时,选择性 SSM 递归 (算法 2) 采取以下形式:

$$
\begin{aligned}
& g_t=\sigma(\operatorname{Linear}(x_t))\\
& h_t=(1-g_t)h_{t-1}+g_tx_t.
\end{aligned}
$$

如 3.2 节所述,我们对 $sΔ,τΔ$ 的特定选择正是源于这种联系。特别需要注意的是,如果给定输入 $x_t$ 应该被完全忽略 (在合成任务中是必需的),所有 D 个通道都应该忽略它,因此我们在重复/广播Δ之前将输入投影到 1 个维度。

### 3.5.2 选择机制的解释

我们详细说明选择的两个特定机制效应。

可变间距。选择性允许过滤掉可能出现在感兴趣输入之间的不相关噪声令牌。这在选择性复制任务中有所体现，但在常见的数据模态中普遍存在，特别是对于离散数据 - 例如语言填充词如 "um" 的存在。这个属性是由于模型可以在机械上过滤掉任何特定的输入 $x_t$，例如在门控 RNN 情况下（定理 1）当 $g_t \rightarrow 0$ 时。

过滤上下文。已经经验性地观察到许多序列模型不随着更长的上下文而改善（F. Shi 等人，2023 年），尽管原则上更多的上下文应该导致更好的性能。一个解释是许多序列模型在需要时无法有效地忽略不相关的上下文；一个直观的例子是全局卷积（和一般的 LTI 模型）。另一方面，选择性模型可以随时重置其状态以去除不相关的历史记录，因此它们的性能原则上会随着上下文长度的增加而单调提高（例如，第 4.3.2 节）。

边界重置。在将多个独立序列拼接在一起的设置中，Transformer 可以通过实例化特定的注意力掩码将它们保持分开，而 LTI 模型会在序列之间泄漏信息。选择性 SSMs 也可以在边界处重置其状态（例如，$\Delta_t \rightarrow \infty$ 或定理 1 中的 $g_t \rightarrow 1$）。这些设置可能是人为的（例如，将文档合并在一起以提高硬件利用率）或自然的（例如，强化学习中的情节边界（Lu 等人，2023 年））。

此外，我们详细说明了每个选择性参数的效应。

解释 $\Delta$。一般来说，$\Delta$ 控制着在多大程度上关注或忽略当前输入 $x_t$ 的平衡。从机械上讲，大的 $\Delta$ 重置状态 $h$ 并专注于当前输入 $x$，而小的 $\Delta$ 保持状态并忽略当前输入。SSMs（1）-（2）可以解释为由时间步长 $\Delta$ 离散化的连续系统，在这种情况下，直观上，大的 $\Delta \rightarrow \infty$ 表示系统更长时间地专注于当前输入（因此“选择”它并忘记其当前状态），而小的 $\Delta \rightarrow 0$ 表示一个被忽略的瞬态输入。

解释 $A$。我们注意到，虽然 $A$ 参数也可以是选择性的，但它最终只通过 $\Delta$ 与 $\bar{A}=\exp (\Delta A)$（离散化（4））的相互作用影响模型。因此，$\Delta$ 中的选择性足以确保 $(\bar{A}, \bar{B})$ 中的选择性，并且是改进的主要来源。我们假设除了（或代替）$\Delta$ 之外，使 $A$ 具有选择性也会产生类似的性能，并且出于简单起见而省略了。

解释 $B$ 和 $C$。如第 3.1 节所讨论的，选择性的最重要属性是过滤掉不相关的信息，以便序列模型的上下文可以被压缩到一个高效的状态中。在 SSM 中，修改 $B$ 和 $C$ 为选择性允许更精细地控制是否将输入 $x_t$ 传入状态 $h_t$ 或将状态传入输出 $y_t$。这可以解释为允许模型根据内容（输入）和上下文（隐藏状态）调节递归动态。

### 3.6 附加模型细节

实数 vs. 复数。大多数先前的 SSMs 在其状态 $h$ 中使用复数，这对许多任务的强大性能是必要的（Gu，Goel 和 Ré，2022 年）。然而，经验性地观察到，完全实数值的 SSMs 似乎表现良好，甚至在某些情况下可能更好（Ma 等人，2023 年）。我们默认使用实数值，这对我们的大多数任务都很有效；我们假设复数和实数之间的权衡与数据模态中的连续 - 离散谱有关，复数有助于连续模态（例如音频、视频），但对于离散模态（例如文本、DNA）则不适用。

初始化。大多数先前的 SSMs 还建议特殊的初始化，特别是在复数值情况下，这可以在诸如低数据情况等几种情况下有所帮助。我们在复数情况下的默认初始化是 S4D-Lin，而在实数情况下是 S4D-Real（Gu，Gupta 等人，2022 年），它基于 HIPPO 理论（Gu，Dao 等人，2020 年）。它们分别将 $\boldsymbol{A}$ 的第 $n$ 个元素定义为 $-1 / 2+n i$ 和 $-(n+1)$。然而，我们期望许多初始化方法都能很好地工作，特别是在大数据和实数值 SSM 的情况下；在第 4.6 节中考虑了一些消融。

$\Delta$ 参数化。我们将 $\Delta$ 的选择性调整定义为 $s_{\Delta}(x)=\operatorname{Broadcast}_D\left(\operatorname{Linear}_1(x)\right)$，这是由 $\Delta$ 的机制所驱动的（第 3.5 节）。我们观察到，它可以从维度 1 泛化到更大的维度 $\mathrm{R}$。我们将其设置为 $\mathrm{D}$ 的一个小分数，与块中主要的线性投影相比，它使用的参数数量可以忽略不计。此外，我们还注意到，广播操作可以被视为另一个线性投影，其初始化为特定模式的 1 和 0；如果这个投影是可训练的，则这导致了替代的 $s_{\Delta}(x)=\operatorname{Linear}_D\left(\operatorname{Linear}_R(x)\right)$，这可以被视为低秩投影。

在我们的实验中，$\Delta$ 参数（可以看作是偏置项）被初始化为 $\tau_{\Delta}^{-1}($ 均匀分布 $([0.001,0.1])$，遵循先前 SSMs 的工作（Gu，Johnson，Timalsina 等人，2023 年）。

注 3.1. 为了简洁起见，我们在实验结果中有时将选择性 SSMs 简称为 S6 模型，因为它们是带有选择机制的 S4 模型，并且是通过扫描计算的。

定理1的证明。考虑一个选择性 SSM (算法 2 )，其中 $N=1, \boldsymbol{A}=-1, \boldsymbol{B}=1, s_{\Delta}=\operatorname{Linear}(x)$, $\tau_{\Delta}=$ softplus。相应的连续时间 SSM (1)为

$$
h(t)=-h(t)+x(t)
$$

这也被称为漏桶积分器。

离散化步长为

$$
\begin{aligned}
\Delta_t & =\tau_{\Delta}\left(\text { Parameter }+s_{\Delta}\left(x_t\right)\right) \\
& =\operatorname{softplus}\left(\text { Parameter }+ \text { Linear }\left(x_t\right)\right) \\
& =\operatorname{softplus}\left(\operatorname{Linear}\left(x_t\right)\right)
\end{aligned}
$$

我们观察到参数可视为可学习偏差并融入线性映射。现在应用零阶保持(ZOH)离散化公式:

$$
\begin{aligned}
\overline{\boldsymbol{A}}_t & =\exp (\Delta \boldsymbol{A})=\frac{1}{1+\exp \left(\operatorname{Linear}\left(x_t\right)\right.}=\sigma\left(-\operatorname{Linear}\left(x_t\right)\right) \\
& =1-\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\
\overline{\boldsymbol{B}}_t & =(\Delta \boldsymbol{A})^{-1}(\exp (\Delta \boldsymbol{A})-\boldsymbol{I}) \cdot \Delta \boldsymbol{B}=-(\exp (\Delta \boldsymbol{A})-\boldsymbol{I})=1-\overline{\boldsymbol{A}} \\
& =\sigma\left(\operatorname{Linear}\left(x_t\right)\right) .
\end{aligned}
$$

因此最终的离散递推(2a)为

$$
\begin{aligned}
& g_t=\sigma\left(\operatorname{Linear}\left(x_t\right)\right) \\
& h_t=\left(1-g_t\right) h_{t-1}+g_t x_t
\end{aligned}
$$

如所需。

